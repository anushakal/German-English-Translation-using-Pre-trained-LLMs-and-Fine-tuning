{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a4cd103",
      "metadata": {
        "id": "3a4cd103"
      },
      "outputs": [],
      "source": [
        "# Author: Roi Yehoshua\n",
        "# Date: January 2024\n",
        "# MIT License\n",
        "\n",
        "# Based on the PyTorch implementation from https://nlp.seas.harvard.edu/annotated-transformer/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the required installations\n",
        "!pip show portalocker\n",
        "!pip show spacy\n",
        "!pip show torchtext"
      ],
      "metadata": {
        "id": "-fN5SPwoWHu2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "704dd3b5-d6a1-468f-e12a-2db67c19da04"
      },
      "id": "-fN5SPwoWHu2",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: portalocker\n",
            "Version: 2.8.2\n",
            "Summary: Wraps the portalocker recipe for easy usage\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: Rick van Hattem <wolph@wol.ph>\n",
            "License: BSD-3-Clause\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: \n",
            "Required-by: \n",
            "Name: spacy\n",
            "Version: 3.7.3\n",
            "Summary: Industrial-strength Natural Language Processing (NLP) in Python\n",
            "Home-page: https://spacy.io\n",
            "Author: Explosion\n",
            "Author-email: contact@explosion.ai\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: catalogue, cymem, jinja2, langcodes, murmurhash, numpy, packaging, preshed, pydantic, requests, setuptools, smart-open, spacy-legacy, spacy-loggers, srsly, thinc, tqdm, typer, wasabi, weasel\n",
            "Required-by: de-core-news-sm, en-core-web-sm, fastai\n",
            "Name: torchtext\n",
            "Version: 0.17.0\n",
            "Summary: Text utilities, models, transforms, and datasets for PyTorch.\n",
            "Home-page: https://github.com/pytorch/text\n",
            "Author: PyTorch Text Team\n",
            "Author-email: packages@pytorch.org\n",
            "License: BSD\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: numpy, requests, torch, torchdata, tqdm\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Upgrading the required packages\n",
        "!pip install portalocker --quiet\n",
        "!pip install spacy --upgrade --quiet\n",
        "!pip install torchtext --upgrade --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxD4vm19WjgE",
        "outputId": "fd07a965-bbf7-4590-dc8d-bf7f26dbec90"
      },
      "id": "rxD4vm19WjgE",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "62a5dee5",
      "metadata": {
        "id": "62a5dee5"
      },
      "outputs": [],
      "source": [
        "#Importing the required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import spacy\n",
        "import os\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import Multi30k\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b17027ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b17027ed",
        "outputId": "5c8f571a-c577-40d0-f0d5-f591629b209f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "#Setting the manual seed and the device\n",
        "torch.manual_seed(42)  # For reproducibility\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffad93f9",
      "metadata": {
        "id": "ffad93f9"
      },
      "source": [
        "### Multi-Head Attention\n",
        "\n",
        "$$\n",
        "    \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O \\\\\n",
        "    \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\\\  \n",
        "    \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bf156177",
      "metadata": {
        "id": "bf156177"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"The multi-head attention module\"\"\"\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        # Ensure the dimension of the model is divisible by the number of heads.\n",
        "        # This is necessary to equally divide the embedding dimension across heads.\n",
        "        assert d_model % num_heads == 0, 'd_model must be divisible by num_heads'\n",
        "\n",
        "        self.d_model = d_model           # Total dimension of the model\n",
        "        self.num_heads = num_heads       # Number of attention heads\n",
        "        self.d_k = d_model // num_heads  # Dimnsion of each head. We assume d_v = d_k\n",
        "\n",
        "        # Linear transformations for queries, keys, and values\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Final linear layer to project the concatenated heads' outputs back to d_model dimensions\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        ### WRITE YOUR CODE HERE\n",
        "\n",
        "        # Calculate attention scores with scaling\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
        "\n",
        "        # Apply mask (if provided)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "        # Apply softmax to attention scores to get probabilities\n",
        "        attention_probs = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Return the weighted sum of values based on attention probabilities\n",
        "        output = torch.matmul(attention_probs, V)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Reshape the input tensor to [batch_size, num_heads, seq_length, d_k]\n",
        "        # to prepare for multi-head attention processing\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        # Inverse operation of split_heads: combine the head outputs back into the original tensor shape\n",
        "        # [batch_size, seq_length, d_model]\n",
        "        batch_size, num_heads, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        ### WRITE YOUR CODE HERE\n",
        "\n",
        "        # Linearly project the queries, keys, and values, and then split them into heads\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "\n",
        "        # scaled dot-product attention for each head\n",
        "        attention_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # Concatenate the heads' outputs\n",
        "        output = self.combine_heads(attention_output)\n",
        "        output = self.W_o(output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70b13867",
      "metadata": {
        "id": "70b13867"
      },
      "source": [
        "### Feed-Forward NN\n",
        "\n",
        "$$\n",
        "    \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5d5ab551",
      "metadata": {
        "id": "5d5ab551"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"\"\"The Positionwise Feedforward Network (FFN) module\"\"\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### WRITE YOUR CODE HERE\n",
        "        # FFN(x) = max(0, xW1 + b1)W2 + b2\n",
        "\n",
        "        # First linear transformation\n",
        "        intermediate_output = self.linear1(x)\n",
        "\n",
        "        # Using the ReLU activation function\n",
        "        intermediate_output = self.relu(intermediate_output)\n",
        "\n",
        "        # Applying dropout\n",
        "        intermediate_output = self.dropout(intermediate_output)\n",
        "\n",
        "        # second linear transformation\n",
        "        output = self.linear2(intermediate_output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4034ef0a",
      "metadata": {
        "id": "4034ef0a"
      },
      "source": [
        "### Positional Encoding\n",
        "\n",
        "$$\n",
        "    \\text{PE}(pos, 2i) = \\sin(pos/10000^{2i/d_{\\text{model}}}) \\\\\n",
        "    \\text{PE}(pos, 2i + 1) = \\cos(pos/10000^{2i/d_{\\text{model}}})\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "bfca835c",
      "metadata": {
        "id": "bfca835c"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the positional encoding module using sinusoidal functions of different frequencies\n",
        "    for each dimension of the encoding.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create a positional encoding (PE) matrix with dimensions [max_seq_length, d_model].\n",
        "        # This matrix will contain the positional encodings for all possible positions up to max_seq_length.\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "\n",
        "        # Generate a tensor of positions (0 to max_seq_length - 1) and reshape it to [max_seq_length, 1].\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # Compute the division term used in the formulas for sin and cos functions.\n",
        "        # This term is based on the dimension of the model and the position, ensuring that the wavelengths\n",
        "        # form a geometric progression from 2π to 10000 * 2π. It uses only even indices for the dimensions.\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Apply the sin function to even indices in the PE matrix. These values are determined by\n",
        "        # multiplying the position by the division term, creating a pattern where each position has\n",
        "        # a unique sinusoidal encoding.\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "        # Apply the cos function to odd indices in the PE matrix, complementing the sin-encoded positions.\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Register 'pe' as a buffer within the module. Unlike parameters, buffers are not updated during training.\n",
        "        # This is crucial because positional encodings are fixed and not subject to training updates.\n",
        "        # The unsqueeze(0) adds a batch dimension for easier broadcasting with input tensors.\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional encoding to the input tensor x.\n",
        "        # x is expected to have dimensions [batch_size, seq_length, d_model].\n",
        "        # The positional encoding 'pe' is sliced to match the seq_length of 'x', and then added to 'x'.\n",
        "        # This operation leverages broadcasting to apply the same positional encoding across the batch.\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96e2475b",
      "metadata": {
        "id": "96e2475b"
      },
      "source": [
        "### Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "000313c6",
      "metadata": {
        "id": "000313c6"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"An encoder layer consists of a multi-head self-attention sublayer and a feed forward sublayer,\n",
        "       with a dropout, residual connection, and layer normalization after each sub-layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        ### WRITE YOUR CODE HERE\n",
        "\n",
        "        # Multi-head self-attention sublayer\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = x + self.dropout(attn_output)\n",
        "        x = self.layer_norm1(x)  # layer normalization\n",
        "\n",
        "        # Feed-forward sublayer\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + self.dropout(ff_output)\n",
        "        x = self.layer_norm2(x)  # layer normalization\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76215e02",
      "metadata": {
        "id": "76215e02"
      },
      "source": [
        "### Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ee27007e",
      "metadata": {
        "id": "ee27007e"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"A decoder layer consists of a multi-head self-attention, cross-attention and a feed-forward sublayers,\n",
        "       with a dropout, residual connection, and layer normalization after each sub-layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        ### WRITE YOUR CODE HERE\n",
        "\n",
        "        # Multi-head self-attention sublayer\n",
        "        self_attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = x + self.dropout(self_attn_output)\n",
        "        x = self.layer_norm1(x)\n",
        "\n",
        "        # Multi-head cross-attention sublayer\n",
        "        cross_attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = x + self.dropout(cross_attn_output)\n",
        "        x = self.layer_norm2(x)\n",
        "\n",
        "        # Feed-forward sublayer\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + self.dropout(ff_output)\n",
        "        x = self.layer_norm3(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ad86614",
      "metadata": {
        "id": "2ad86614"
      },
      "source": [
        "### The Full Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "31af1f2a",
      "metadata": {
        "id": "31af1f2a"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, N, n_heads, d_ff, max_seq_length, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding layers for source and target\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        # Encoder and Decoder stacks\n",
        "        self.encoder = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(N)])\n",
        "        self.decoder = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(N)])\n",
        "\n",
        "        # Output linear layer\n",
        "        self.out = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Initialization\n",
        "        self.init_weights()\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"Initialize parameters with Glorot / fan_avg\"\"\"\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def create_source_mask(self, src):\n",
        "        \"\"\"Create masks for both padding tokens and future tokens\"\"\"\n",
        "        # Source padding mask\n",
        "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, src_len]\n",
        "        # unsqueeze(1) adds a dimension for the heads of the multi-head attention\n",
        "        # unsqueeze(2) adds a dimension for the attention scores\n",
        "        # This mask can be broadcasted across the src_len dimension of the attention scores,\n",
        "        # effectively masking out specific tokens across all heads and all positions in the sequence.\n",
        "        return src_mask\n",
        "\n",
        "    def create_target_mask(self, tgt):\n",
        "        # Target padding mask\n",
        "        tgt_pad_mask = (tgt != self.pad_idx).unsqueeze(1).unsqueeze(3)  # [batch_size, 1, tgt_len, 1]\n",
        "        # unsqueeze(1) adds a dimension for the heads of the multi-head attention\n",
        "        # unsqueeze(3) adds a dimension for the attention scores\n",
        "        # The final shape allows the mask to be broadcast across the attention scores, ensuring positions only\n",
        "        # attend to allowed positions as dictated by the no-peak mask (the preceding positions) and the padding mask.\n",
        "\n",
        "        # Target no-peak mask\n",
        "        tgt_len = tgt.size(1)\n",
        "        tgt_nopeak_mask = torch.tril(torch.ones(tgt_len, tgt_len, device=device)).bool()\n",
        "\n",
        "        # Combine masks\n",
        "        tgt_mask = tgt_pad_mask & tgt_nopeak_mask  # [batch_size, 1, tgt_len, tgt_len]\n",
        "        return tgt_mask\n",
        "\n",
        "    def encode(self, src):\n",
        "        \"\"\"Encodes the source sequence using the Transformer encoder stack.\n",
        "        \"\"\"\n",
        "        src_mask = self.create_source_mask(src)\n",
        "        src = self.dropout(self.positional_encoding(self.src_embedding(src)))\n",
        "\n",
        "        # Pass through each layer in the encoder\n",
        "        for layer in self.encoder:\n",
        "            src = layer(src, src_mask)\n",
        "        return src, src_mask\n",
        "\n",
        "    def decode(self, tgt, memory, src_mask):\n",
        "        \"\"\"Decodes the target sequence using the Transformer decoder stack, given the memory from the encoder.\n",
        "        \"\"\"\n",
        "        tgt_mask = self.create_target_mask(tgt)\n",
        "        tgt = self.dropout(self.positional_encoding(self.tgt_embedding(tgt)))\n",
        "\n",
        "        # Pass through each layer in the decoder\n",
        "        for layer in self.decoder:\n",
        "            tgt = layer(tgt, memory, src_mask, tgt_mask)\n",
        "\n",
        "        # Output layer\n",
        "        output = self.out(tgt)\n",
        "        return output\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        ### WRITE YOUR CODE HERE\n",
        "\n",
        "        # Encoding\n",
        "        memory, source_mask = self.encode(src)\n",
        "\n",
        "        # Decoding\n",
        "        output = self.decode(tgt, memory, source_mask)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "11a3b60d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11a3b60d",
        "outputId": "7c1347d7-f28c-4669-96cd-58d0ac655ed6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (src_embedding): Embedding(5000, 512)\n",
              "  (tgt_embedding): Embedding(5000, 512)\n",
              "  (positional_encoding): PositionalEncoding()\n",
              "  (encoder): ModuleList(\n",
              "    (0-5): 6 x EncoderLayer(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): PositionwiseFeedForward(\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (decoder): ModuleList(\n",
              "    (0-5): 6 x DecoderLayer(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (cross_attn): MultiHeadAttention(\n",
              "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): PositionwiseFeedForward(\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (out): Linear(in_features=512, out_features=5000, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Define the hyperparameters of the model\n",
        "src_vocab_size = 5000  # Size of source vocabulary\n",
        "tgt_vocab_size = 5000  # Size of target vocabulary\n",
        "d_model = 512          # Embedding dimension\n",
        "N = 6                  # Number of encoder and decoder layers\n",
        "num_heads = 8          # Number of attention heads\n",
        "d_ff = 2048            # Dimension of feed forward networks\n",
        "max_seq_length = 100   # Maximum sequence length\n",
        "dropout = 0.1          # Dropout rate\n",
        "pad_idx = 0            # Index of the padding token\n",
        "\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
        "\n",
        "# Move the model to the appropriate device (GPU or CPU)\n",
        "model = model.to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "050f7248",
      "metadata": {
        "id": "050f7248"
      },
      "source": [
        "### Testing on Random Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "149dadd1",
      "metadata": {
        "id": "149dadd1"
      },
      "outputs": [],
      "source": [
        "# Generate random sample data\n",
        "torch.manual_seed(42)\n",
        "\n",
        "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)\n",
        "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d5b1c56",
      "metadata": {
        "id": "2d5b1c56"
      },
      "source": [
        "#### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "45583975",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45583975",
        "outputId": "07092079-ae1e-4729-b9b8-12cfab457c3d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([990], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Generate the next token using the first token in the first target tensor\n",
        "model.eval()\n",
        "\n",
        "memory, src_mask = model.encode(src_data[:1, :])\n",
        "output = model.decode(tgt_data[:1, :1], memory, src_mask)\n",
        "y = output.view(-1, tgt_vocab_size).argmax(-1)\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18b7e1c5",
      "metadata": {
        "id": "18b7e1c5"
      },
      "source": [
        "If your code is correct, you should get tensor([990])."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a314148e",
      "metadata": {
        "id": "a314148e"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2afd8ff3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2afd8ff3",
        "outputId": "7a1f14d0-a02c-4461-c22c-504069bff99b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 8.604100227355957\n",
            "Epoch: 2, Loss: 8.504327774047852\n",
            "Epoch: 3, Loss: 8.372350692749023\n",
            "Epoch: 4, Loss: 8.297316551208496\n",
            "Epoch: 5, Loss: 8.23608112335205\n",
            "Epoch: 6, Loss: 8.194172859191895\n",
            "Epoch: 7, Loss: 8.163470268249512\n",
            "Epoch: 8, Loss: 8.141220092773438\n",
            "Epoch: 9, Loss: 8.127995491027832\n",
            "Epoch: 10, Loss: 8.11793327331543\n"
          ]
        }
      ],
      "source": [
        "# Train the model for 10 epochs\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "model.train()\n",
        "\n",
        "n_epochs = 10\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(src_data, tgt_data[:, :-1])\n",
        "\n",
        "    # tgt_data is of shape [batch_size, tgt_len]\n",
        "    # output is of shape [batch_size, tgt_len, tgt_vocab_size]\n",
        "    output = output.contiguous().view(-1, tgt_vocab_size)\n",
        "    tgt = tgt_data[:, 1:].contiguous().view(-1)\n",
        "    loss = criterion(output, tgt)\n",
        "\n",
        "    loss.backward()\n",
        "    grad_clip = 1.0\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    optimizer.step()\n",
        "    print(f'Epoch: {epoch + 1}, Loss: {loss.item()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42f36f48",
      "metadata": {
        "id": "42f36f48"
      },
      "source": [
        "You should see the loss decreasing from around 8.6 to 8.1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71caa7ed",
      "metadata": {
        "id": "71caa7ed"
      },
      "source": [
        "### Machine Translation Example\n",
        "\n",
        "Now we consider a real-world example using the Multi30k German-English Translation task. This task is much smaller than the WMT task considered in the paper, but it illustrates the whole system. <br>\n",
        "It is recommended to run this example on Google Colab, or on a machine with a strong GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c38e67f4",
      "metadata": {
        "id": "c38e67f4"
      },
      "source": [
        "#### Define Tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "08f20abd",
      "metadata": {
        "id": "08f20abd"
      },
      "outputs": [],
      "source": [
        "# Load spacy models for tokenization\n",
        "try:\n",
        "    spacy_de = spacy.load('de_core_news_sm')\n",
        "except IOError:\n",
        "    os.system(\"python -m spacy download de_core_news_sm\")\n",
        "    spacy_de = spacy.load('de_core_news_sm')\n",
        "\n",
        "try:\n",
        "    spacy_en = spacy.load('en_core_web_sm')\n",
        "except IOError:\n",
        "    os.system(\"python -m spacy download en_core_web_sm\")\n",
        "    spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenize_de(text):\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "def yield_tokens(data_iter, tokenizer, language):\n",
        "    for data_sample in data_iter:\n",
        "        yield tokenizer(data_sample[language])\n",
        "\n",
        "tokenizer_de = get_tokenizer(tokenize_de)\n",
        "tokenizer_en = get_tokenizer(tokenize_en)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e1c9820",
      "metadata": {
        "id": "0e1c9820"
      },
      "source": [
        "#### Build Vocabularies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7474b5e6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7474b5e6",
        "outputId": "59fcdc33-82c9-4575-ec5a-2234899133ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        }
      ],
      "source": [
        "train_data, _, _ = Multi30k(split=('train', 'valid', 'test'))\n",
        "vocab_src = build_vocab_from_iterator(yield_tokens(train_data, tokenizer_de, 0), specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "vocab_tgt = build_vocab_from_iterator(yield_tokens(train_data, tokenizer_en, 1), specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "\n",
        "vocab_src.set_default_index(vocab_src['<unk>'])\n",
        "vocab_tgt.set_default_index(vocab_tgt['<unk>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "914ca38f",
      "metadata": {
        "id": "914ca38f"
      },
      "source": [
        "#### Create the Transformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_transformer_model(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx):\n",
        "    \"\"\"\n",
        "    Initializes the Transformer model based on the provided hyperparameters.\n",
        "\n",
        "    Parameters:\n",
        "    - src_vocab_size (int): Size of source vocabulary\n",
        "    - tgt_vocab_size (int): Size of target vocabulary\n",
        "    - d_model (int): Embedding dimension\n",
        "    - N (int): Number of encoder and decoder layers\n",
        "    - num_heads (int): Number of attention heads\n",
        "    - d_ff (int): Dimension of feed forward networks\n",
        "    - max_seq_length (int): Maximum sequence length\n",
        "    - dropout (float): Dropout rate\n",
        "    - pad_idx (int): Padding index in the target vocabulary\n",
        "\n",
        "    Returns:\n",
        "    - Transformer: Initialized Transformer model\n",
        "    \"\"\"\n",
        "    model = Transformer(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    return model"
      ],
      "metadata": {
        "id": "Udod_XiVC6hn"
      },
      "id": "Udod_XiVC6hn",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "4c862170",
      "metadata": {
        "id": "4c862170"
      },
      "outputs": [],
      "source": [
        "# Define the hyperparameters of the model\n",
        "src_vocab_size = len(vocab_src)  # Size of source vocabulary\n",
        "tgt_vocab_size = len(vocab_tgt)  # Size of target vocabulary\n",
        "d_model = 512  # Embedding dimension\n",
        "N = 6          # Number of encoder and decoder layers\n",
        "num_heads = 8  # Number of attention heads\n",
        "d_ff = 2048    # Dimension of feed forward networks\n",
        "max_seq_length = 5000 # Maximum sequence length\n",
        "dropout = 0.1  # Dropout rate\n",
        "learning_rate = 0.0001 #Learning Rate\n",
        "batch_size = 128\n",
        "grad_clip = 1\n",
        "\n",
        "# Assume pad_idx is the padding index in the target vocabulary\n",
        "pad_idx = vocab_tgt['<pad>']\n",
        "\n",
        "default_model = initialize_transformer_model(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
        "\n",
        "optimizer = optim.Adam(default_model.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Initialize the loss function with CrossEntropyLoss, ignoring the padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0aa58a6e",
      "metadata": {
        "id": "0aa58a6e"
      },
      "source": [
        "#### Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "3ca10d94",
      "metadata": {
        "id": "3ca10d94"
      },
      "outputs": [],
      "source": [
        "def data_process(raw_data_iter):\n",
        "    data = []\n",
        "    for raw_src, raw_tgt in raw_data_iter:\n",
        "        src_tensor = torch.tensor([vocab_src[token] for token in tokenizer_de(raw_src)], dtype=torch.long)\n",
        "        tgt_tensor = torch.tensor([vocab_tgt[token] for token in tokenizer_en(raw_tgt)], dtype=torch.long)\n",
        "        data.append((src_tensor, tgt_tensor))\n",
        "    return data\n",
        "\n",
        "train_data, valid_data, test_data = Multi30k(split=('train', 'valid', 'test'))\n",
        "train_data = data_process(train_data)\n",
        "valid_data = data_process(valid_data)\n",
        "#test_data = data_process(test_data)\n",
        "# The test set of Multi30k is corrupted\n",
        "# See https://discuss.pytorch.org/t/unicodedecodeerror-when-running-test-iterator/192818/3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "1c2f452a",
      "metadata": {
        "id": "1c2f452a"
      },
      "outputs": [],
      "source": [
        "def generate_batch(data_batch):\n",
        "    \"\"\"Processes a batch of source-target pairs by adding start-of-sequence (BOS) and end-of-sequence (EOS) tokens\n",
        "    to each sequence and padding all sequences to the same length.\n",
        "\n",
        "    Parameters:\n",
        "    - data_batch (Iterable[Tuple[Tensor, Tensor]]): A batch of source-target pairs, where each element is a tuple\n",
        "      containing the source sequence tensor and the target sequence tensor.\n",
        "    \"\"\"\n",
        "    src_batch, tgt_batch = [], []\n",
        "    src_batch, tgt_batch = [], []\n",
        "\n",
        "    # Iterate over each source-target pair in the provided batch\n",
        "    for src_item, tgt_item in data_batch:\n",
        "        # Prepend the start-of-sequence (BOS) token and append the end-of-sequence (EOS) token to the sequences\n",
        "        src_batch.append(torch.cat([torch.tensor([vocab_src['<bos>']]), src_item,\n",
        "                                    torch.tensor([vocab_src['<eos>']])], dim=0))\n",
        "        tgt_batch.append(torch.cat([torch.tensor([vocab_tgt['<bos>']]), tgt_item,\n",
        "                                    torch.tensor([vocab_tgt['<eos>']])], dim=0))\n",
        "\n",
        "    # Pad the sequences in the source batch to ensure they all have the same length.\n",
        "    # 'batch_first=True' indicates that the batch dimension should come first in the resulting tensor.\n",
        "    src_batch = pad_sequence(src_batch, padding_value=vocab_src['<pad>'], batch_first=True)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=vocab_tgt['<pad>'], batch_first=True)\n",
        "    return src_batch, tgt_batch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader for the training data, using the generate_batch function as the collate_fn.\n",
        "# This allows custom processing of each batch (adding BOS/EOS tokens and padding) before being fed into the model.\n",
        "train_iterator = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
        "\n",
        "# Similarly, DataLoader for the validation data\n",
        "valid_iterator = DataLoader(valid_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)"
      ],
      "metadata": {
        "id": "DQUbwdfBZMZa"
      },
      "id": "DQUbwdfBZMZa",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "2cbd8431",
      "metadata": {
        "id": "2cbd8431"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, grad_clip):\n",
        "    \"\"\"\n",
        "    Trains the model for one epoch over the given dataset.\n",
        "    This function iterates over the provided data iterator, performing the forward and backward passes for each batch.\n",
        "    It employs teacher forcing by feeding the shifted target sequence (excluding the last token) as input to the decoder.\n",
        "\n",
        "    Parameters:\n",
        "    - model (torch.nn.Module): The model to be trained.\n",
        "    - iterator (Iterable): An iterable object that returns batches of data.\n",
        "    - optimizer (torch.optim.Optimizer): The optimizer to use for updating the model parameters.\n",
        "    - criterion (Callable): The loss function used to compute the difference between the model's predictions and the actual targets.\n",
        "    - grad_clip (float): The maximum norm of the gradients for gradient clipping.\n",
        "\n",
        "    Returns:\n",
        "    - float: The average loss for the epoch, computed as the total loss over all batches divided by the number of batches in the iterator.\n",
        "    \"\"\"\n",
        "    # Set the model to training mode.\n",
        "    # This enables dropout, layer normalization etc., which behave differently during training.\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # Enumerate over the data iterator to get batches\n",
        "    for i, batch in enumerate(iterator):\n",
        "        # Unpack the batch to get source (src) and target (tgt) sequences\n",
        "        src, tgt = batch\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass through the model.\n",
        "        # For seq2seq models, the decoder input (tgt[:, :-1]) excludes the last token, implementing teacher forcing.\n",
        "        output = model(src, tgt[:, :-1])\n",
        "\n",
        "        # Reshape the output and target tensors to compute loss.\n",
        "        # The output tensor is reshaped to a 2D tensor where rows correspond to each token in the batch and columns to vocabulary size.\n",
        "\n",
        "        # tgt is of shape [batch_size, tgt_len]\n",
        "        # output is of shape [batch_size, tgt_len, tgt_vocab_size]\n",
        "        output = output.contiguous().view(-1, tgt_vocab_size)\n",
        "\n",
        "        # The target tensor is reshaped to a 1D tensor, excluding the first token (BOS) from each sequence.\n",
        "        tgt = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        # Compute loss, perform backpropagation, and update model parameters\n",
        "        loss = criterion(output, tgt)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Compute average loss per batch for the current epoch\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "31d18844",
      "metadata": {
        "id": "31d18844"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \"\"\"\n",
        "    Evaluates the model's performance on a given dataset.\n",
        "    This function is similar to the training loop, but without the backward pass and parameter updates. I\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src, tgt = batch\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            output = model(src, tgt[:, :-1])\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            tgt = tgt[:, 1:].contiguous().view(-1)\n",
        "            loss = criterion(output, tgt)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0287e5dd",
      "metadata": {
        "id": "0287e5dd"
      },
      "source": [
        "#### Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "d02463b8",
      "metadata": {
        "id": "d02463b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32420cd2-984d-4e32-e7f6-2ad8a29e388f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the default model\n",
            "\n",
            "\n",
            "Epoch: 1\n",
            "\tTrain Loss: 5.692\n",
            "\tVal Loss: 4.985\n",
            "\n",
            "Epoch: 2\n",
            "\tTrain Loss: 4.880\n",
            "\tVal Loss: 4.793\n",
            "\n",
            "Epoch: 3\n",
            "\tTrain Loss: 4.697\n",
            "\tVal Loss: 4.628\n",
            "\n",
            "Epoch: 4\n",
            "\tTrain Loss: 4.426\n",
            "\tVal Loss: 4.245\n",
            "\n",
            "Epoch: 5\n",
            "\tTrain Loss: 4.102\n",
            "\tVal Loss: 4.012\n",
            "\n",
            "Epoch: 6\n",
            "\tTrain Loss: 3.896\n",
            "\tVal Loss: 3.869\n",
            "\n",
            "Epoch: 7\n",
            "\tTrain Loss: 3.759\n",
            "\tVal Loss: 3.783\n",
            "\n",
            "Epoch: 8\n",
            "\tTrain Loss: 3.637\n",
            "\tVal Loss: 3.676\n",
            "\n",
            "Epoch: 9\n",
            "\tTrain Loss: 3.529\n",
            "\tVal Loss: 3.600\n",
            "\n",
            "Epoch: 10\n",
            "\tTrain Loss: 3.450\n",
            "\tVal Loss: 3.555\n",
            "\n",
            "Epoch: 11\n",
            "\tTrain Loss: 3.379\n",
            "\tVal Loss: 3.505\n",
            "\n",
            "Epoch: 12\n",
            "\tTrain Loss: 3.317\n",
            "\tVal Loss: 3.484\n",
            "\n",
            "Epoch: 13\n",
            "\tTrain Loss: 3.259\n",
            "\tVal Loss: 3.430\n",
            "\n",
            "Epoch: 14\n",
            "\tTrain Loss: 3.198\n",
            "\tVal Loss: 3.390\n",
            "\n",
            "Epoch: 15\n",
            "\tTrain Loss: 3.137\n",
            "\tVal Loss: 3.331\n",
            "\n",
            "Epoch: 16\n",
            "\tTrain Loss: 3.081\n",
            "\tVal Loss: 3.302\n",
            "\n",
            "Epoch: 17\n",
            "\tTrain Loss: 3.025\n",
            "\tVal Loss: 3.268\n",
            "\n",
            "Epoch: 18\n",
            "\tTrain Loss: 2.972\n",
            "\tVal Loss: 3.238\n",
            "\n",
            "Epoch: 19\n",
            "\tTrain Loss: 2.922\n",
            "\tVal Loss: 3.197\n",
            "\n",
            "Epoch: 20\n",
            "\tTrain Loss: 2.876\n",
            "\tVal Loss: 3.175\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 20\n",
        "print(\"Training the default model\\n\")\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(default_model, train_iterator, optimizer, criterion, grad_clip)\n",
        "    val_loss = evaluate(default_model, valid_iterator, criterion)\n",
        "\n",
        "    print(f'\\nEpoch: {epoch + 1}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tVal Loss: {val_loss:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64a844bb",
      "metadata": {
        "id": "64a844bb"
      },
      "source": [
        "The train loss should decrease from around 5.7 to 2.8 after 20 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the model"
      ],
      "metadata": {
        "id": "kHqJqQBnEKEr"
      },
      "id": "kHqJqQBnEKEr"
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(path, model, optimizer, epoch, train_loss, val_loss, src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx):\n",
        "    # Save the model's state dictionary, optimizer state, and other necessary information\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'train_loss': train_loss,\n",
        "        'val_loss': val_loss,\n",
        "        'src_vocab_size': src_vocab_size,\n",
        "        'tgt_vocab_size': tgt_vocab_size,\n",
        "        'd_model': d_model,\n",
        "        'N': N,\n",
        "        'num_heads': num_heads,\n",
        "        'd_ff': d_ff,\n",
        "        'max_seq_length': max_seq_length,\n",
        "        'dropout': dropout,\n",
        "        'pad_idx': pad_idx\n",
        "    }, path)\n",
        "\n",
        "    print(f'Model saved to {path}')"
      ],
      "metadata": {
        "id": "Lycjyg46ELj3"
      },
      "id": "Lycjyg46ELj3",
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path where you want to save the model\n",
        "save_path = '/content/sample_data/model_default.pth'"
      ],
      "metadata": {
        "id": "uDe-fLRVkvNd"
      },
      "id": "uDe-fLRVkvNd",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(save_path, default_model, optimizer, n_epochs, train_loss, val_loss, src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_JQX3bhdOcl",
        "outputId": "d46825b0-66d8-4890-dc0d-61b32c2f5b26"
      },
      "id": "y_JQX3bhdOcl",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/sample_data/model_default.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(path):\n",
        "    # Load the checkpoint\n",
        "    loaded_checkpoint = torch.load(path)\n",
        "\n",
        "    # Create a new instance of the model using the parameters from the checkpoint\n",
        "    loaded_model = Transformer(\n",
        "    loaded_checkpoint['src_vocab_size'],\n",
        "    loaded_checkpoint['tgt_vocab_size'],\n",
        "    loaded_checkpoint['d_model'],\n",
        "    loaded_checkpoint['N'],\n",
        "    loaded_checkpoint['num_heads'],\n",
        "    loaded_checkpoint['d_ff'],\n",
        "    loaded_checkpoint['max_seq_length'],\n",
        "    loaded_checkpoint['dropout'],\n",
        "    loaded_checkpoint['pad_idx'])\n",
        "\n",
        "    # Load the state dictionaries into the model and optimizer\n",
        "    loaded_model.load_state_dict(loaded_checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(loaded_checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    #Moving loaded model to device\n",
        "    loaded_model = loaded_model.to(device)\n",
        "\n",
        "    return loaded_model\n"
      ],
      "metadata": {
        "id": "-2iFeWS3GLZc"
      },
      "id": "-2iFeWS3GLZc",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_path = \"/content/sample_data/model_default.pth\""
      ],
      "metadata": {
        "id": "whDsJ8lX9Fub"
      },
      "id": "whDsJ8lX9Fub",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "default_model = load_model(load_path)"
      ],
      "metadata": {
        "id": "uhV9gc7j9KTT"
      },
      "id": "uhV9gc7j9KTT",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "817a7300",
      "metadata": {
        "id": "817a7300"
      },
      "source": [
        "#### Translating a Sample Sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "0b1bd36c",
      "metadata": {
        "id": "0b1bd36c"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(model, sentence, vocab_src, vocab_tgt, max_length=50):\n",
        "    \"\"\"\n",
        "    Translates a given source sentence into the target language using a trained Transformer model.\n",
        "    The function preprocesses the input sentence by tokenizing and converting it to tensor format, then uses the model's\n",
        "    encode and decode methods to generate the translated sentence. The translation process is performed token by token\n",
        "    using greedy decoding, selecting the most likely next token at each step until an <eos> token is produced or the\n",
        "    maximum length is reached.\n",
        "\n",
        "    Parameters:\n",
        "    - model (torch.nn.Module): The trained Transformer model.\n",
        "    - sentence (str): The source sentence to translate.\n",
        "    - vocab_src (dict): The source vocabulary mapping of tokens to indices. It should include special tokens such as\n",
        "      '<bos>' (beginning of sentence) and '<eos>' (end of sentence).\n",
        "    - vocab_tgt (dict): The target vocabulary mapping of indices to tokens. It should provide a method `lookup_token`\n",
        "      to convert token indices back to the string representation.\n",
        "    - max_length (int, optional): The maximum allowed length for the generated translation. The decoding process will\n",
        "      stop when this length is reached if an <eos> token has not yet been generated.\n",
        "\n",
        "    Returns:\n",
        "    - str: The translated sentence as a string of text in the target language.\n",
        "    \"\"\"\n",
        "    ### WRITE YOUR CODE HERE\n",
        "\n",
        "    # Tokenize and convert source sentence to tensor\n",
        "    src_tensor = torch.tensor([vocab_src[token] for token in tokenizer_de(sentence)], dtype=torch.long)\n",
        "\n",
        "    # Move tensor to device\n",
        "    src_tensor = src_tensor.to(device)\n",
        "\n",
        "    # Encoding the source sentence\n",
        "    with torch.no_grad():\n",
        "        encoder_output, src_mask = model.encode(src_tensor.unsqueeze(0))\n",
        "\n",
        "\n",
        "    tgt_tokens = []\n",
        "\n",
        "    # Input to begin with BOS token\n",
        "    input_token = torch.tensor([vocab_tgt['<bos>']], dtype=torch.long).to(device)\n",
        "\n",
        "    # decoding using greedy approach\n",
        "    for _ in range(max_length):\n",
        "        with torch.no_grad():\n",
        "            output = model.decode(input_token.unsqueeze(0), encoder_output, src_mask)\n",
        "            next_token = output.argmax(dim=-1)[:, -1].item()\n",
        "\n",
        "        tgt_tokens.append(next_token)\n",
        "\n",
        "        # Stop decoding if EOS token is generated\n",
        "        if next_token == vocab_tgt['<eos>']:\n",
        "            break\n",
        "\n",
        "        # Prepare the next input token\n",
        "        input_token = torch.tensor([next_token], dtype=torch.long).to(device)\n",
        "\n",
        "    translated_sentence = ' '.join([vocab_tgt.lookup_token(token) for token in tgt_tokens])\n",
        "\n",
        "    return translated_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "438aff1f",
      "metadata": {
        "id": "438aff1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6d15ef0-0876-48c5-abb7-7887d6213cea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: A little person in field A little person in field A little person in field A little person in field A little person in field A little person in field A little person in field A little person in field A little person in field A little person in field\n"
          ]
        }
      ],
      "source": [
        "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
        "translated_sentence = translate_sentence(default_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Die Sonne scheint hell am blauen Himmel.\"  # German for \"The sun is shining brightly in the blue sky.\"\n",
        "translated_sentence = translate_sentence(default_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2IA3NCsvl7p",
        "outputId": "563a8739-c654-4ca0-e8fb-196cb588ad6c"
      },
      "id": "z2IA3NCsvl7p",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: People in race People in race People in race People in race People in race People in race People in race People in race People in race People in race People in race People in race People in race People in race People in race People in race People in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Ich liebe es, Zeit in der Natur zu verbringen, umgeben von Bäumen und Blumen.\"  # German for \"I love spending time in nature, surrounded by trees and flowers.\"\n",
        "translated_sentence = translate_sentence(default_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI-pkCzevoV4",
        "outputId": "b7f23db1-982c-4534-def0-b1abd28d4e62"
      },
      "id": "ZI-pkCzevoV4",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: People bike People bike People bike People bike People bike People bike People bike People bike People bike People bike People bike People bike People bike People bike People bike People bike People bike People bike People bike People bike People bike People bike People bike People bike People bike\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Können Sie ein gutes Restaurant im Stadtzentrum empfehlen.\"  # German for \"Can you recommend a good restaurant in the city center.\"\n",
        "translated_sentence = translate_sentence(default_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e2mF_IQvo5x",
        "outputId": "b7c2d955-aef6-4b27-96f4-f7006d47f327"
      },
      "id": "1e2mF_IQvo5x",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Wir planen eine Reise nach Deutschland nächsten Sommer.\"  # German for \"We are planning a trip to Germany next summer.\"\n",
        "translated_sentence = translate_sentence(default_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rC6IiK_LvpVg",
        "outputId": "380e1d80-1758-47ad-853e-ac7de95ff0be"
      },
      "id": "rC6IiK_LvpVg",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Gib mir bitte das Salz und den Pfeffer.\"  # German for \"Please pass me the salt and pepper.\"\n",
        "translated_sentence = translate_sentence(default_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q81U7MDavq67",
        "outputId": "53e04519-8b41-4fee-ed06-734237ac27c4"
      },
      "id": "q81U7MDavq67",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Guten Tag.\"  # German for \"Good morning.\"\n",
        "translated_sentence = translate_sentence(default_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCXECMxrO5H0",
        "outputId": "9ca24bb1-8e14-4878-d3ec-a8981c57ae9b"
      },
      "id": "zCXECMxrO5H0",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: Children <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec6be352",
      "metadata": {
        "id": "ec6be352"
      },
      "source": [
        "You should get a translation similar to the reference after 20 epochs of training."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Translate sentence using Beam Search"
      ],
      "metadata": {
        "id": "zJB4Gt983wpH"
      },
      "id": "zJB4Gt983wpH"
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(model, sentence, vocab_src, vocab_tgt, max_length=50, beam_size=5):\n",
        "    # Tokenize and convert source sentence to tensor\n",
        "    src_tensor = torch.tensor([vocab_src[token] for token in tokenizer_de(sentence)], dtype=torch.long)\n",
        "\n",
        "    # Move tensor to device\n",
        "    src_tensor = src_tensor.to(device)\n",
        "\n",
        "    # Encode the source sentence\n",
        "    with torch.no_grad():\n",
        "        encoder_output, src_mask = model.encode(src_tensor.unsqueeze(0))\n",
        "\n",
        "    # Initialise with <bos>\n",
        "    beam = [(torch.tensor([vocab_tgt['<bos>']], dtype=torch.long).to(device), 0.0)]\n",
        "\n",
        "    # Perform beam search\n",
        "    for _ in range(max_length):\n",
        "        candidates = []\n",
        "\n",
        "        for seq, score in beam:\n",
        "            output = model.decode(seq.unsqueeze(0), encoder_output, src_mask)\n",
        "\n",
        "            # Get the top beam_size candidates for the next token\n",
        "            top_candidates = torch.topk(output[:, -1, :], k=beam_size, dim=-1)\n",
        "\n",
        "            for i in range(beam_size):\n",
        "                next_token = top_candidates.indices[0, i].item()\n",
        "                next_score = top_candidates.values[0, i].item()\n",
        "\n",
        "                # Add the candidate sequence and its score to the list\n",
        "                candidates.append((torch.cat([seq, torch.tensor([next_token], dtype=torch.long).to(device)]), score + next_score))\n",
        "\n",
        "        # Select the top beam_size candidates based on their accumulated scores\n",
        "        def get_score(candidate):\n",
        "          return candidate[1]\n",
        "\n",
        "        # Select the top beam_size candidates based on their accumulated scores\n",
        "        beam = sorted(candidates, key=get_score, reverse=True)[:beam_size]\n",
        "\n",
        "        # Check if any candidate has an EOS token\n",
        "        for seq, score in beam:\n",
        "            if seq[-1].item() == vocab_tgt['<eos>']:\n",
        "                return ' '.join([vocab_tgt.lookup_token(token.item()) for token in seq])\n",
        "\n",
        "    # If max_length is reached and no EOS token is generated, return the best candidate\n",
        "    return ' '.join([vocab_tgt.lookup_token(token.item()) for token, _ in beam[0]])\n"
      ],
      "metadata": {
        "id": "aP7cuIrH5ZLN"
      },
      "id": "aP7cuIrH5ZLN",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
        "translated_sentence = beam_search(default_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDh-nV8b3z-B",
        "outputId": "c085a358-31ad-43b3-fde5-21cb367ab68b"
      },
      "id": "eDh-nV8b3z-B",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: <bos> A person is playing in the water . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Die Sonne scheint hell am blauen Himmel.\"  # German for \"The sun is shining brightly in the blue sky.\"\n",
        "translated_sentence = beam_search(default_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dF5HlIPA5nIW",
        "outputId": "41cd8a10-e044-42bc-9966-056274ff1bd1"
      },
      "id": "dF5HlIPA5nIW",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: <bos> Several people are sitting in the water . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Ich liebe es, Zeit in der Natur zu verbringen, umgeben von Bäumen und Blumen.\"  # German for \"I love spending time in nature, surrounded by trees and flowers.\"\n",
        "translated_sentence = beam_search(default_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdMerBNi5z4d",
        "outputId": "a24aa726-07e6-45cc-82d8-27cd0e936cf0"
      },
      "id": "mdMerBNi5z4d",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: <bos> People are walking down the street in a city . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Wir planen eine Reise nach Deutschland nächsten Sommer.\"  # German for \"We are planning a trip to Germany next summer.\"\n",
        "translated_sentence = beam_search(default_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BB6RTmpO56_P",
        "outputId": "73f62ad0-4414-47a7-b673-fa2e4026b9cd"
      },
      "id": "BB6RTmpO56_P",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: <bos> A group of people are walking down the street . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Gib mir bitte das Salz und den Pfeffer.\"  # German for \"Please pass me the salt and pepper.\"\n",
        "translated_sentence = beam_search(default_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lijg3ac695W0",
        "outputId": "42be6d2b-8a8c-41b7-863a-3afda41c9d00"
      },
      "id": "lijg3ac695W0",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: <bos> A group of people are walking down the street . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Guten Tag.\"  # German for \"Good Morning.\"\n",
        "translated_sentence = beam_search(default_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK97Ckk_PJRN",
        "outputId": "fb1c61fd-6d16-4bd0-c84b-e060515c4399"
      },
      "id": "yK97Ckk_PJRN",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: <bos> Three people are playing soccer . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Experimenting with hyperparameters"
      ],
      "metadata": {
        "id": "vw8KSz9a5uWV"
      },
      "id": "vw8KSz9a5uWV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Increasing no of epochs first from 20 to 30"
      ],
      "metadata": {
        "id": "l6Xw1ZT8-JIq"
      },
      "id": "l6Xw1ZT8-JIq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameters of the model\n",
        "src_vocab_size = len(vocab_src)  # Size of source vocabulary\n",
        "tgt_vocab_size = len(vocab_tgt)  # Size of target vocabulary\n",
        "d_model = 512  # Embedding dimension\n",
        "N = 6          # Number of encoder and decoder layers\n",
        "num_heads = 8  # Number of attention heads\n",
        "d_ff = 2048    # Dimension of feed forward networks\n",
        "max_seq_length = 5000 # Maximum sequence length\n",
        "dropout = 0.1  # Dropout rate\n",
        "learning_rate = 0.0001 #Learning Rate\n",
        "batch_size = 128\n",
        "grad_clip = 1\n",
        "\n",
        "# Assume pad_idx is the padding index in the target vocabulary\n",
        "pad_idx = vocab_tgt['<pad>']\n",
        "\n",
        "epochs_model = initialize_transformer_model(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
        "\n",
        "optimizer = optim.Adam(epochs_model.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Initialize the loss function with CrossEntropyLoss, ignoring the padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "metadata": {
        "id": "BNBTLG0EI29L"
      },
      "id": "BNBTLG0EI29L",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader for the training data, using the generate_batch function as the collate_fn.\n",
        "# This allows custom processing of each batch (adding BOS/EOS tokens and padding) before being fed into the model.\n",
        "train_iterator = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
        "\n",
        "# Similarly, DataLoader for the validation data\n",
        "valid_iterator = DataLoader(valid_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)"
      ],
      "metadata": {
        "id": "1ChsBIoxJCcQ"
      },
      "id": "1ChsBIoxJCcQ",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 30\n",
        "print(\"Training the default model for more epochs\\n\")\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(epochs_model, train_iterator, optimizer, criterion, grad_clip)\n",
        "    val_loss = evaluate(epochs_model, valid_iterator, criterion)\n",
        "\n",
        "    print(f'\\nEpoch: {epoch + 1}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tVal Loss: {val_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGVArzP894os",
        "outputId": "29c40250-3192-400a-ebe2-f12b0370372d"
      },
      "id": "TGVArzP894os",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the default model for more epochs\n",
            "\n",
            "\n",
            "Epoch: 1\n",
            "\tTrain Loss: 5.715\n",
            "\tVal Loss: 5.022\n",
            "\n",
            "Epoch: 2\n",
            "\tTrain Loss: 4.881\n",
            "\tVal Loss: 4.781\n",
            "\n",
            "Epoch: 3\n",
            "\tTrain Loss: 4.695\n",
            "\tVal Loss: 4.621\n",
            "\n",
            "Epoch: 4\n",
            "\tTrain Loss: 4.437\n",
            "\tVal Loss: 4.261\n",
            "\n",
            "Epoch: 5\n",
            "\tTrain Loss: 4.113\n",
            "\tVal Loss: 4.052\n",
            "\n",
            "Epoch: 6\n",
            "\tTrain Loss: 3.925\n",
            "\tVal Loss: 3.897\n",
            "\n",
            "Epoch: 7\n",
            "\tTrain Loss: 3.790\n",
            "\tVal Loss: 3.805\n",
            "\n",
            "Epoch: 8\n",
            "\tTrain Loss: 3.664\n",
            "\tVal Loss: 3.709\n",
            "\n",
            "Epoch: 9\n",
            "\tTrain Loss: 3.561\n",
            "\tVal Loss: 3.635\n",
            "\n",
            "Epoch: 10\n",
            "\tTrain Loss: 3.474\n",
            "\tVal Loss: 3.581\n",
            "\n",
            "Epoch: 11\n",
            "\tTrain Loss: 3.396\n",
            "\tVal Loss: 3.515\n",
            "\n",
            "Epoch: 12\n",
            "\tTrain Loss: 3.329\n",
            "\tVal Loss: 3.483\n",
            "\n",
            "Epoch: 13\n",
            "\tTrain Loss: 3.264\n",
            "\tVal Loss: 3.426\n",
            "\n",
            "Epoch: 14\n",
            "\tTrain Loss: 3.198\n",
            "\tVal Loss: 3.392\n",
            "\n",
            "Epoch: 15\n",
            "\tTrain Loss: 3.138\n",
            "\tVal Loss: 3.339\n",
            "\n",
            "Epoch: 16\n",
            "\tTrain Loss: 3.083\n",
            "\tVal Loss: 3.304\n",
            "\n",
            "Epoch: 17\n",
            "\tTrain Loss: 3.027\n",
            "\tVal Loss: 3.288\n",
            "\n",
            "Epoch: 18\n",
            "\tTrain Loss: 2.971\n",
            "\tVal Loss: 3.233\n",
            "\n",
            "Epoch: 19\n",
            "\tTrain Loss: 2.920\n",
            "\tVal Loss: 3.197\n",
            "\n",
            "Epoch: 20\n",
            "\tTrain Loss: 2.867\n",
            "\tVal Loss: 3.180\n",
            "\n",
            "Epoch: 21\n",
            "\tTrain Loss: 2.817\n",
            "\tVal Loss: 3.142\n",
            "\n",
            "Epoch: 22\n",
            "\tTrain Loss: 2.767\n",
            "\tVal Loss: 3.119\n",
            "\n",
            "Epoch: 23\n",
            "\tTrain Loss: 2.722\n",
            "\tVal Loss: 3.103\n",
            "\n",
            "Epoch: 24\n",
            "\tTrain Loss: 2.677\n",
            "\tVal Loss: 3.077\n",
            "\n",
            "Epoch: 25\n",
            "\tTrain Loss: 2.633\n",
            "\tVal Loss: 3.051\n",
            "\n",
            "Epoch: 26\n",
            "\tTrain Loss: 2.590\n",
            "\tVal Loss: 3.033\n",
            "\n",
            "Epoch: 27\n",
            "\tTrain Loss: 2.546\n",
            "\tVal Loss: 3.002\n",
            "\n",
            "Epoch: 28\n",
            "\tTrain Loss: 2.502\n",
            "\tVal Loss: 2.979\n",
            "\n",
            "Epoch: 29\n",
            "\tTrain Loss: 2.460\n",
            "\tVal Loss: 2.975\n",
            "\n",
            "Epoch: 30\n",
            "\tTrain Loss: 2.417\n",
            "\tVal Loss: 2.953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
        "translated_sentence = translate_sentence(epochs_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMGhDGg_56XC",
        "outputId": "1b07ad73-3791-4dbe-a662-ff0684a807a0"
      },
      "id": "wMGhDGg_56XC",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: A African A African A African A African A African A African A African A African A African A African A African A African A African A African A African A African A African A African A African A African A African A African A African A African A African\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
        "translated_sentence = beam_search(epochs_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVgF-NZ85zHi",
        "outputId": "687a4756-830c-401a-ff76-0dd0f86f51c5"
      },
      "id": "gVgF-NZ85zHi",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: <bos> The boy is playing on the beach . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Die Sonne scheint hell am blauen Himmel.\"  # German for \"The sun is shining brightly in the blue sky.\"\n",
        "translated_sentence = translate_sentence(epochs_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zix71lrTJAG",
        "outputId": "20abea49-a21f-4c10-dab7-e2a045b94e6a"
      },
      "id": "5zix71lrTJAG",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: People Several People Several People Several People Several People Several People Several People Several People Several People Several People Several People Several People Several People Several People Several People Several People Several People Several People Several People Several People Several People Several People Several People Several People Several People Several\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Die Sonne scheint hell am blauen Himmel.\"  # German for \"The sun is shining brightly in the blue sky.\"\n",
        "translated_sentence = beam_search(epochs_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54hvGUVjTUke",
        "outputId": "7071e4f4-c6e8-455b-ddb9-ffeaf42b3088"
      },
      "id": "54hvGUVjTUke",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: <bos> A group of three men are walking . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Ich liebe es, Zeit in der Natur zu verbringen, umgeben von Bäumen und Blumen.\"  # German for \"I love spending time in nature, surrounded by trees and flowers.\"\n",
        "translated_sentence = translate_sentence(epochs_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNwjo3rGTl9x",
        "outputId": "feba8fa0-274b-4be1-99bf-f66ee74dab11"
      },
      "id": "XNwjo3rGTl9x",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Ich liebe es, Zeit in der Natur zu verbringen, umgeben von Bäumen und Blumen.\"  # German for \"I love spending time in nature, surrounded by trees and flowers.\"\n",
        "translated_sentence = beam_search(epochs_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ksWiF7lTY8r",
        "outputId": "6d78df14-3d45-4f60-ed7f-4f9d9c9bff79"
      },
      "id": "3ksWiF7lTY8r",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: <bos> Construction players in the middle of a football game in a game . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Wir planen eine Reise nach Deutschland nächsten Sommer.\"  # German for \"We are planning a trip to Germany next summer.\"\n",
        "translated_sentence = translate_sentence(epochs_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNYqxILjTvDd",
        "outputId": "723937aa-d3f8-451d-e60e-cae0ffdb8701"
      },
      "id": "PNYqxILjTvDd",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: Workers a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Wir planen eine Reise nach Deutschland nächsten Sommer.\"  # German for \"We are planning a trip to Germany next summer.\"\n",
        "translated_sentence = beam_search(epochs_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSTB5wIsT1gT",
        "outputId": "bdcb4a0d-6605-4e94-954d-3d169aec29be"
      },
      "id": "zSTB5wIsT1gT",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: <bos> Construction workers are working on a road . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Gib mir bitte das Salz und den Pfeffer.\"  # German for \"Please pass me the salt and pepper.\"\n",
        "translated_sentence = translate_sentence(epochs_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNLAQ0vhT_Ij",
        "outputId": "6d31a122-9c0b-415d-c209-dfd3a5ccd659"
      },
      "id": "cNLAQ0vhT_Ij",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: Workers a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Gib mir bitte das Salz und den Pfeffer.\"  # German for \"Please pass me the salt and pepper.\"\n",
        "translated_sentence = beam_search(epochs_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEKBb26JUp2C",
        "outputId": "3a5313a5-d8fd-47b0-8860-7f574b2b7514"
      },
      "id": "KEKBb26JUp2C",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: <bos> Construction workers are working on a bench . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Guten Tag.\"  # German for \"Good Morning.\"\n",
        "translated_sentence = translate_sentence(epochs_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIqpCayxU2XZ",
        "outputId": "927111d4-db11-413b-e80f-320ca25d909e"
      },
      "id": "dIqpCayxU2XZ",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Guten Tag.\"  # German for \"Good Morning.\"\n",
        "translated_sentence = beam_search(epochs_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfzIhtfDUtG5",
        "outputId": "7b75e437-5cf1-4910-a4fe-9f2617b16c8d"
      },
      "id": "YfzIhtfDUtG5",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: <bos> <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Changing batch size from 128 to 64"
      ],
      "metadata": {
        "id": "nwuuz4OBMOD5"
      },
      "id": "nwuuz4OBMOD5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameters of the model\n",
        "src_vocab_size = len(vocab_src)  # Size of source vocabulary\n",
        "tgt_vocab_size = len(vocab_tgt)  # Size of target vocabulary\n",
        "d_model = 512  # Embedding dimension\n",
        "N = 6          # Number of encoder and decoder layers\n",
        "num_heads = 8  # Number of attention heads\n",
        "d_ff = 2048    # Dimension of feed forward networks\n",
        "max_seq_length = 5000 # Maximum sequence length\n",
        "dropout = 0.1  # Dropout rate\n",
        "learning_rate = 0.0001 #Learning Rate\n",
        "batch_size = 64\n",
        "grad_clip = 1\n",
        "\n",
        "# Assume pad_idx is the padding index in the target vocabulary\n",
        "pad_idx = vocab_tgt['<pad>']\n",
        "\n",
        "batch_64_model = initialize_transformer_model(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
        "\n",
        "optimizer = optim.Adam(batch_64_model.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Initialize the loss function with CrossEntropyLoss, ignoring the padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "metadata": {
        "id": "5-zhFo6w8pdq"
      },
      "id": "5-zhFo6w8pdq",
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader for the training data, using the generate_batch function as the collate_fn.\n",
        "# This allows custom processing of each batch (adding BOS/EOS tokens and padding) before being fed into the model.\n",
        "train_iterator = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
        "\n",
        "# Similarly, DataLoader for the validation data\n",
        "valid_iterator = DataLoader(valid_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)"
      ],
      "metadata": {
        "id": "v3pONB2iHn2W"
      },
      "id": "v3pONB2iHn2W",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 20\n",
        "print(\"Training the model with a batch size of 64\")\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(batch_64_model, train_iterator, optimizer, criterion, grad_clip)\n",
        "    val_loss = evaluate(batch_64_model, valid_iterator, criterion)\n",
        "\n",
        "    print(f'\\nEpoch: {epoch + 1}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tVal Loss: {val_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJLzXUUGHoVG",
        "outputId": "70b992bf-66bd-4a72-e41b-f5cb813dbd9d"
      },
      "id": "MJLzXUUGHoVG",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model with a batch size of 64\n",
            "\n",
            "Epoch: 1\n",
            "\tTrain Loss: 5.367\n",
            "\tVal Loss: 4.980\n",
            "\n",
            "Epoch: 2\n",
            "\tTrain Loss: 4.743\n",
            "\tVal Loss: 4.675\n",
            "\n",
            "Epoch: 3\n",
            "\tTrain Loss: 4.371\n",
            "\tVal Loss: 4.173\n",
            "\n",
            "Epoch: 4\n",
            "\tTrain Loss: 3.986\n",
            "\tVal Loss: 3.949\n",
            "\n",
            "Epoch: 5\n",
            "\tTrain Loss: 3.788\n",
            "\tVal Loss: 3.778\n",
            "\n",
            "Epoch: 6\n",
            "\tTrain Loss: 3.656\n",
            "\tVal Loss: 3.677\n",
            "\n",
            "Epoch: 7\n",
            "\tTrain Loss: 3.556\n",
            "\tVal Loss: 3.597\n",
            "\n",
            "Epoch: 8\n",
            "\tTrain Loss: 3.459\n",
            "\tVal Loss: 3.525\n",
            "\n",
            "Epoch: 9\n",
            "\tTrain Loss: 3.379\n",
            "\tVal Loss: 3.477\n",
            "\n",
            "Epoch: 10\n",
            "\tTrain Loss: 3.315\n",
            "\tVal Loss: 3.432\n",
            "\n",
            "Epoch: 11\n",
            "\tTrain Loss: 3.254\n",
            "\tVal Loss: 3.405\n",
            "\n",
            "Epoch: 12\n",
            "\tTrain Loss: 3.196\n",
            "\tVal Loss: 3.342\n",
            "\n",
            "Epoch: 13\n",
            "\tTrain Loss: 3.135\n",
            "\tVal Loss: 3.313\n",
            "\n",
            "Epoch: 14\n",
            "\tTrain Loss: 3.079\n",
            "\tVal Loss: 3.275\n",
            "\n",
            "Epoch: 15\n",
            "\tTrain Loss: 3.023\n",
            "\tVal Loss: 3.215\n",
            "\n",
            "Epoch: 16\n",
            "\tTrain Loss: 2.970\n",
            "\tVal Loss: 3.178\n",
            "\n",
            "Epoch: 17\n",
            "\tTrain Loss: 2.923\n",
            "\tVal Loss: 3.143\n",
            "\n",
            "Epoch: 18\n",
            "\tTrain Loss: 2.875\n",
            "\tVal Loss: 3.134\n",
            "\n",
            "Epoch: 19\n",
            "\tTrain Loss: 2.827\n",
            "\tVal Loss: 3.100\n",
            "\n",
            "Epoch: 20\n",
            "\tTrain Loss: 2.781\n",
            "\tVal Loss: 3.087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
        "translated_sentence = translate_sentence(batch_64_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfOP5aMRIxMP",
        "outputId": "8757a754-b620-48f3-eb52-851de9d7a0e9"
      },
      "id": "hfOP5aMRIxMP",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: A boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy boy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
        "translated_sentence = beam_search(batch_64_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjoCDStgVY7Q",
        "outputId": "9cf210ac-dbc1-403a-f8d0-1e0b8b869a88"
      },
      "id": "yjoCDStgVY7Q",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: <bos> A young boy playing in the water . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Die Sonne scheint hell am blauen Himmel.\"  # German for \"The sun is shining brightly in the blue sky.\"\n",
        "translated_sentence = translate_sentence(batch_64_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyb0ucbsLbbb",
        "outputId": "65b476b2-d3ed-4185-bc38-ef2aa75d6f10"
      },
      "id": "uyb0ucbsLbbb",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: People and construction People and construction People and construction People and construction People and construction People and construction People and construction People and construction People and construction People and construction People and construction People and construction People and construction People and construction People and construction People and construction People and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Die Sonne scheint hell am blauen Himmel.\"  # German for \"The sun is shining brightly in the blue sky.\"\n",
        "translated_sentence = beam_search(batch_64_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebM0UgXHWYKr",
        "outputId": "0a8fd1bc-02da-44ab-9353-be342333e1be"
      },
      "id": "ebM0UgXHWYKr",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: <bos> A group of men are standing in front of a building <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Ich liebe es, Zeit in der Natur zu verbringen, umgeben von Bäumen und Blumen.\"  # German for \"I love spending time in nature, surrounded by trees and flowers.\"\n",
        "translated_sentence = translate_sentence(batch_64_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Dlxj-N7Lgig",
        "outputId": "c9c25705-0104-4066-91bc-4e65128d976d"
      },
      "id": "-Dlxj-N7Lgig",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: The People this People this People this People this People this People this People this People this People this People this People this People this People this People this People this People this People this People this People this People this People this People this People this People this People\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Ich liebe es, Zeit in der Natur zu verbringen, umgeben von Bäumen und Blumen.\"  # German for \"I love spending time in nature, surrounded by trees and flowers.\"\n",
        "translated_sentence = beam_search(batch_64_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7C41R1VcLygg",
        "outputId": "1ffa1ebe-2ef0-4741-fa10-3ca4dd3a04e3"
      },
      "id": "7C41R1VcLygg",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: <bos> People are walking down a city street in front of a building . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Wir planen eine Reise nach Deutschland nächsten Sommer.\"  # German for \"We are planning a trip to Germany next summer.\"\n",
        "translated_sentence = translate_sentence(batch_64_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxKgoLz6WuzI",
        "outputId": "49e70a98-3ea9-49a6-dcbb-a2ac06d1677d"
      },
      "id": "wxKgoLz6WuzI",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: The Children The Children The Children The Children The Children The Children The Children The Children The Children The Children The Children The Children The Children The Children The Children The Children The Children The Children The Children The Children The Children The Children The Children The Children The Children\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Wir planen eine Reise nach Deutschland nächsten Sommer.\"  # German for \"We are planning a trip to Germany next summer.\"\n",
        "translated_sentence = beam_search(batch_64_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMqraqoVWu-R",
        "outputId": "adb4636c-34a2-41ca-e117-1867da36bf77"
      },
      "id": "lMqraqoVWu-R",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: <bos> Children are walking down the street . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Gib mir bitte das Salz und den Pfeffer.\"  # German for \"Please pass me the salt and pepper.\"\n",
        "translated_sentence = translate_sentence(batch_64_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_xeSAbtXQ6s",
        "outputId": "9d3c0e80-d1a7-4011-db95-c5d3118d03f4"
      },
      "id": "I_xeSAbtXQ6s",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: The Construction The Construction The Construction The Construction The Construction The Construction The Construction The Construction The Construction The Construction The Construction The Construction The Construction The Construction The Construction The Construction The Construction The Construction The Construction The Construction The Construction The Construction The Construction The Construction The Construction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Gib mir bitte das Salz und den Pfeffer.\"  # German for \"Please pass me the salt and pepper.\"\n",
        "translated_sentence = beam_search(batch_64_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Db-zbRH_W-_2",
        "outputId": "f9fcd267-12c5-43d6-a379-becf3196191e"
      },
      "id": "Db-zbRH_W-_2",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: <bos> Children are walking down a street . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Guten Tag.\"  # German for \"Good Morning.\"\n",
        "translated_sentence = translate_sentence(batch_64_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jivtwT0AXXZG",
        "outputId": "cdb4ecad-fd61-47ba-ec64-061053080893"
      },
      "id": "jivtwT0AXXZG",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three Three\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Guten Tag.\"  # German for \"Good Morning.\"\n",
        "translated_sentence = beam_search(batch_64_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMFQW7XIXXlN",
        "outputId": "14ab46ca-cb73-4d45-be5c-2fd3b8f015f0"
      },
      "id": "lMFQW7XIXXlN",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: <bos> <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/sample_data/model_batch_size64.pth'\n",
        "save_model(save_path, batch_64_model, optimizer, n_epochs, train_loss, val_loss, src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ku-TYgL6L3wT",
        "outputId": "f1466a17-0d6f-421b-bbea-eb5ca40bddb9"
      },
      "id": "ku-TYgL6L3wT",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/sample_data/model_batch_size64.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.1 Increasing batch size to 256"
      ],
      "metadata": {
        "id": "jlbr6ewaRK5q"
      },
      "id": "jlbr6ewaRK5q"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameters of the model\n",
        "src_vocab_size = len(vocab_src)  # Size of source vocabulary\n",
        "tgt_vocab_size = len(vocab_tgt)  # Size of target vocabulary\n",
        "d_model = 512  # Embedding dimension\n",
        "N = 6          # Number of encoder and decoder layers\n",
        "num_heads = 8  # Number of attention heads\n",
        "d_ff = 2048    # Dimension of feed forward networks\n",
        "max_seq_length = 5000 # Maximum sequence length\n",
        "dropout = 0.1  # Dropout rate\n",
        "learning_rate = 0.0001 #Learning Rate\n",
        "batch_size = 256\n",
        "grad_clip = 1\n",
        "\n",
        "# Assume pad_idx is the padding index in the target vocabulary\n",
        "pad_idx = vocab_tgt['<pad>']\n",
        "\n",
        "batch_256_model = initialize_transformer_model(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
        "\n",
        "optimizer = optim.Adam(batch_256_model.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Initialize the loss function with CrossEntropyLoss, ignoring the padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "metadata": {
        "id": "nrZmHLOJRNbT"
      },
      "id": "nrZmHLOJRNbT",
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader for the training data, using the generate_batch function as the collate_fn.\n",
        "# This allows custom processing of each batch (adding BOS/EOS tokens and padding) before being fed into the model.\n",
        "train_iterator = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
        "\n",
        "# Similarly, DataLoader for the validation data\n",
        "valid_iterator = DataLoader(valid_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)"
      ],
      "metadata": {
        "id": "wG5ztGPKRSum"
      },
      "id": "wG5ztGPKRSum",
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 20\n",
        "print(\"Training the model for batch size 256\")\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(batch_256_model, train_iterator, optimizer, criterion, grad_clip)\n",
        "    val_loss = evaluate(batch_256_model, valid_iterator, criterion)\n",
        "\n",
        "    print(f'\\nEpoch: {epoch + 1}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tVal Loss: {val_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TP1gx-rjRZfy",
        "outputId": "feb2bb38-3b99-4add-e483-16ddb16f48ce"
      },
      "id": "TP1gx-rjRZfy",
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model for batch size 256\n",
            "\n",
            "Epoch: 1\n",
            "\tTrain Loss: 6.277\n",
            "\tVal Loss: 5.155\n",
            "\n",
            "Epoch: 2\n",
            "\tTrain Loss: 5.064\n",
            "\tVal Loss: 4.966\n",
            "\n",
            "Epoch: 3\n",
            "\tTrain Loss: 4.892\n",
            "\tVal Loss: 4.820\n",
            "\n",
            "Epoch: 4\n",
            "\tTrain Loss: 4.766\n",
            "\tVal Loss: 4.723\n",
            "\n",
            "Epoch: 5\n",
            "\tTrain Loss: 4.638\n",
            "\tVal Loss: 4.569\n",
            "\n",
            "Epoch: 6\n",
            "\tTrain Loss: 4.435\n",
            "\tVal Loss: 4.288\n",
            "\n",
            "Epoch: 7\n",
            "\tTrain Loss: 4.198\n",
            "\tVal Loss: 4.122\n",
            "\n",
            "Epoch: 8\n",
            "\tTrain Loss: 4.040\n",
            "\tVal Loss: 4.034\n",
            "\n",
            "Epoch: 9\n",
            "\tTrain Loss: 3.904\n",
            "\tVal Loss: 3.897\n",
            "\n",
            "Epoch: 10\n",
            "\tTrain Loss: 3.795\n",
            "\tVal Loss: 3.819\n",
            "\n",
            "Epoch: 11\n",
            "\tTrain Loss: 3.694\n",
            "\tVal Loss: 3.753\n",
            "\n",
            "Epoch: 12\n",
            "\tTrain Loss: 3.609\n",
            "\tVal Loss: 3.674\n",
            "\n",
            "Epoch: 13\n",
            "\tTrain Loss: 3.539\n",
            "\tVal Loss: 3.632\n",
            "\n",
            "Epoch: 14\n",
            "\tTrain Loss: 3.470\n",
            "\tVal Loss: 3.580\n",
            "\n",
            "Epoch: 15\n",
            "\tTrain Loss: 3.406\n",
            "\tVal Loss: 3.540\n",
            "\n",
            "Epoch: 16\n",
            "\tTrain Loss: 3.344\n",
            "\tVal Loss: 3.490\n",
            "\n",
            "Epoch: 17\n",
            "\tTrain Loss: 3.286\n",
            "\tVal Loss: 3.471\n",
            "\n",
            "Epoch: 18\n",
            "\tTrain Loss: 3.232\n",
            "\tVal Loss: 3.419\n",
            "\n",
            "Epoch: 19\n",
            "\tTrain Loss: 3.181\n",
            "\tVal Loss: 3.379\n",
            "\n",
            "Epoch: 20\n",
            "\tTrain Loss: 3.133\n",
            "\tVal Loss: 3.355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
        "translated_sentence = translate_sentence(batch_256_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLY7WLWqW8HG",
        "outputId": "015f7827-cf4a-421f-c120-2f59f63c8257"
      },
      "id": "zLY7WLWqW8HG",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
        "translated_sentence = beam_search(batch_256_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyBMjpKNYhFO",
        "outputId": "91f85089-1813-420b-858b-7350f0947e65"
      },
      "id": "tyBMjpKNYhFO",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: <bos> The little girl is playing in the snow . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Die Sonne scheint hell am blauen Himmel.\"  # German for \"The sun is shining brightly in the blue sky.\"\n",
        "translated_sentence = translate_sentence(batch_256_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qboAZgbsXIQn",
        "outputId": "dfe79a34-c9c9-4d5c-bc49-a0c172645dea"
      },
      "id": "qboAZgbsXIQn",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Die Sonne scheint hell am blauen Himmel.\"  # German for \"The sun is shining brightly in the blue sky.\"\n",
        "translated_sentence = beam_search(batch_256_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6NMipCXXKur",
        "outputId": "ca9e65bb-0600-454a-c8cd-cbac04bd3faa"
      },
      "id": "X6NMipCXXKur",
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: <bos> People are walking on the street . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Ich liebe es, Zeit in der Natur zu verbringen, umgeben von Bäumen und Blumen.\"  # German for \"I love spending time in nature, surrounded by trees and flowers.\"\n",
        "translated_sentence = translate_sentence(batch_256_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJYod_sjZHoB",
        "outputId": "36f7bb81-f46f-4bf5-8f82-9ac24dd0238d"
      },
      "id": "TJYod_sjZHoB",
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Ich liebe es, Zeit in der Natur zu verbringen, umgeben von Bäumen und Blumen.\"  # German for \"I love spending time in nature, surrounded by trees and flowers.\"\n",
        "translated_sentence = beam_search(batch_256_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdGgV9d1ZKyU",
        "outputId": "8a6a3123-2dfc-4ed0-950b-58d369493a40"
      },
      "id": "SdGgV9d1ZKyU",
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: <bos> A large group of people are walking down a city street in the background . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Wir planen eine Reise nach Deutschland nächsten Sommer.\"  # German for \"We are planning a trip to Germany next summer.\"\n",
        "translated_sentence = translate_sentence(batch_256_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cENa15Q4ZURp",
        "outputId": "e51e3840-4998-404f-8a9f-a3fe91931a44"
      },
      "id": "cENa15Q4ZURp",
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Wir planen eine Reise nach Deutschland nächsten Sommer.\"  # German for \"We are planning a trip to Germany next summer.\"\n",
        "translated_sentence = beam_search(batch_256_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gpC5PA7ZWSm",
        "outputId": "41c9c5f9-86e0-4db3-e9fa-bb006d1ddd88"
      },
      "id": "7gpC5PA7ZWSm",
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: <bos> A group of people are walking down the street . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Gib mir bitte das Salz und den Pfeffer.\"  # German for \"Please pass me the salt and pepper.\"\n",
        "translated_sentence = translate_sentence(batch_256_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6lkxubJZzUU",
        "outputId": "3d0f6c05-1cc2-4d94-81a6-71d97efff523"
      },
      "id": "z6lkxubJZzUU",
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People A People\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Gib mir bitte das Salz und den Pfeffer.\"  # German for \"Please pass me the salt and pepper.\"\n",
        "translated_sentence = beam_search(batch_256_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBnZYeaqZ1kX",
        "outputId": "ab6d6402-958a-486e-c143-5586a22bf401"
      },
      "id": "sBnZYeaqZ1kX",
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: <bos> A group of young men are sitting on a street . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Guten Tag.\"  # German for \"Good Morning.\"\n",
        "translated_sentence = translate_sentence(batch_256_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHx8dAoIZ5Yy",
        "outputId": "cacabc74-30ff-46e9-d9db-377f711ffd36"
      },
      "id": "sHx8dAoIZ5Yy",
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Guten Tag.\"  # German for \"Good Morning.\"\n",
        "translated_sentence = beam_search(batch_256_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRzuRoKuZ9m1",
        "outputId": "04a87adc-716c-4218-da0d-d8625399482b"
      },
      "id": "wRzuRoKuZ9m1",
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: <bos> <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/sample_data/model_batch_size256.pth'\n",
        "save_model(save_path, batch_64_model, optimizer, n_epochs, train_loss, val_loss, src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uP0SJuhjaK_e",
        "outputId": "c5a988bf-c444-40a4-c867-afc2f4ff211d"
      },
      "id": "uP0SJuhjaK_e",
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/sample_data/model_batch_size256.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Changing learning rate to 0.0005"
      ],
      "metadata": {
        "id": "RxU1HW9aMSRa"
      },
      "id": "RxU1HW9aMSRa"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameters of the model\n",
        "src_vocab_size = len(vocab_src)  # Size of source vocabulary\n",
        "tgt_vocab_size = len(vocab_tgt)  # Size of target vocabulary\n",
        "d_model = 512  # Embedding dimension\n",
        "N = 6          # Number of encoder and decoder layers\n",
        "num_heads = 8  # Number of attention heads\n",
        "d_ff = 2048    # Dimension of feed forward networks\n",
        "max_seq_length = 5000 # Maximum sequence length\n",
        "dropout = 0.1  # Dropout rate\n",
        "learning_rate = 0.0005 #Learning Rate\n",
        "batch_size = 128\n",
        "grad_clip = 1\n",
        "\n",
        "# Assume pad_idx is the padding index in the target vocabulary\n",
        "pad_idx = vocab_tgt['<pad>']\n",
        "\n",
        "lr_1_model = initialize_transformer_model(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
        "\n",
        "optimizer = optim.Adam(lr_1_model.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Initialize the loss function with CrossEntropyLoss, ignoring the padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "metadata": {
        "id": "-FMVVZyrMVx0"
      },
      "id": "-FMVVZyrMVx0",
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader for the training data, using the generate_batch function as the collate_fn.\n",
        "# This allows custom processing of each batch (adding BOS/EOS tokens and padding) before being fed into the model.\n",
        "train_iterator = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
        "\n",
        "# Similarly, DataLoader for the validation data\n",
        "valid_iterator = DataLoader(valid_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)"
      ],
      "metadata": {
        "id": "CSJAtF4UbKwA"
      },
      "id": "CSJAtF4UbKwA",
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 20\n",
        "print(\"Training the model for learning rate: 0.0005\")\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(lr_1_model, train_iterator, optimizer, criterion, grad_clip)\n",
        "    val_loss = evaluate(lr_1_model, valid_iterator, criterion)\n",
        "\n",
        "    print(f'\\nEpoch: {epoch + 1}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tVal Loss: {val_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gb7KW_02NIGK",
        "outputId": "c256d1fa-526b-40a6-e4cf-22c82a37ca83"
      },
      "id": "Gb7KW_02NIGK",
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model for learning rate: 0.0005\n",
            "\n",
            "Epoch: 1\n",
            "\tTrain Loss: 5.758\n",
            "\tVal Loss: 7.390\n",
            "\n",
            "Epoch: 2\n",
            "\tTrain Loss: 5.283\n",
            "\tVal Loss: 12.580\n",
            "\n",
            "Epoch: 3\n",
            "\tTrain Loss: 5.198\n",
            "\tVal Loss: 12.137\n",
            "\n",
            "Epoch: 4\n",
            "\tTrain Loss: 5.175\n",
            "\tVal Loss: 12.135\n",
            "\n",
            "Epoch: 5\n",
            "\tTrain Loss: 5.143\n",
            "\tVal Loss: 12.101\n",
            "\n",
            "Epoch: 6\n",
            "\tTrain Loss: 5.139\n",
            "\tVal Loss: 12.218\n",
            "\n",
            "Epoch: 7\n",
            "\tTrain Loss: 5.111\n",
            "\tVal Loss: 11.185\n",
            "\n",
            "Epoch: 8\n",
            "\tTrain Loss: 5.101\n",
            "\tVal Loss: 11.893\n",
            "\n",
            "Epoch: 9\n",
            "\tTrain Loss: 5.088\n",
            "\tVal Loss: 11.418\n",
            "\n",
            "Epoch: 10\n",
            "\tTrain Loss: 5.079\n",
            "\tVal Loss: 11.628\n",
            "\n",
            "Epoch: 11\n",
            "\tTrain Loss: 5.072\n",
            "\tVal Loss: 10.750\n",
            "\n",
            "Epoch: 12\n",
            "\tTrain Loss: 5.065\n",
            "\tVal Loss: 10.850\n",
            "\n",
            "Epoch: 13\n",
            "\tTrain Loss: 5.060\n",
            "\tVal Loss: 11.144\n",
            "\n",
            "Epoch: 14\n",
            "\tTrain Loss: 5.057\n",
            "\tVal Loss: 11.898\n",
            "\n",
            "Epoch: 15\n",
            "\tTrain Loss: 5.050\n",
            "\tVal Loss: 12.287\n",
            "\n",
            "Epoch: 16\n",
            "\tTrain Loss: 5.041\n",
            "\tVal Loss: 11.730\n",
            "\n",
            "Epoch: 17\n",
            "\tTrain Loss: 5.027\n",
            "\tVal Loss: 12.418\n",
            "\n",
            "Epoch: 18\n",
            "\tTrain Loss: 4.998\n",
            "\tVal Loss: 12.215\n",
            "\n",
            "Epoch: 19\n",
            "\tTrain Loss: 4.915\n",
            "\tVal Loss: 11.197\n",
            "\n",
            "Epoch: 20\n",
            "\tTrain Loss: 4.855\n",
            "\tVal Loss: 11.701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
        "translated_sentence = translate_sentence(lr_1_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkO98aOkPvkR",
        "outputId": "2110be36-0e0c-47bf-c181-a6c87b1646de"
      },
      "id": "bkO98aOkPvkR",
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Die Sonne scheint hell am blauen Himmel.\"  # German for \"The sun is shining brightly in the blue sky.\"\n",
        "translated_sentence = translate_sentence(lr_1_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjWB4G-DPwV6",
        "outputId": "36591058-bee0-4850-e578-6ab4738d8ad5"
      },
      "id": "BjWB4G-DPwV6",
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Ich liebe es, Zeit in der Natur zu verbringen, umgeben von Bäumen und Blumen.\"  # German for \"I love spending time in nature, surrounded by trees and flowers.\"\n",
        "translated_sentence = translate_sentence(lr_1_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HyIqcv-byN1",
        "outputId": "50cbf094-3592-428f-a6f0-1a2d30f49041"
      },
      "id": "0HyIqcv-byN1",
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Wir planen eine Reise nach Deutschland nächsten Sommer.\"  # German for \"We are planning a trip to Germany next summer.\"\n",
        "translated_sentence = translate_sentence(lr_1_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y47BRKxIb9aF",
        "outputId": "d034dff6-b218-4116-8977-036bcdbdeec1"
      },
      "id": "Y47BRKxIb9aF",
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Gib mir bitte das Salz und den Pfeffer.\"  # German for \"Please pass me the salt and pepper.\"\n",
        "translated_sentence = translate_sentence(lr_1_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtC1U8sNcb1Q",
        "outputId": "e5506702-888d-4fb1-fb9e-54dc2f1b51d2"
      },
      "id": "HtC1U8sNcb1Q",
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Guten Tag.\"  # German for \"Good Morning.\"\n",
        "translated_sentence = translate_sentence(lr_1_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZxQkxKQcsnH",
        "outputId": "5172acb3-0177-453e-9cbf-b4edc4152f1c"
      },
      "id": "PZxQkxKQcsnH",
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Changing dropout to 0.2"
      ],
      "metadata": {
        "id": "jl2f7e1KeZNP"
      },
      "id": "jl2f7e1KeZNP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameters of the model\n",
        "src_vocab_size = len(vocab_src)  # Size of source vocabulary\n",
        "tgt_vocab_size = len(vocab_tgt)  # Size of target vocabulary\n",
        "d_model = 512  # Embedding dimension\n",
        "N = 6          # Number of encoder and decoder layers\n",
        "num_heads = 8  # Number of attention heads\n",
        "d_ff = 2048    # Dimension of feed forward networks\n",
        "max_seq_length = 5000 # Maximum sequence length\n",
        "dropout = 0.2  # Dropout rate\n",
        "learning_rate = 0.0001 #Learning Rate\n",
        "batch_size = 128\n",
        "grad_clip = 1\n",
        "\n",
        "# Assume pad_idx is the padding index in the target vocabulary\n",
        "pad_idx = vocab_tgt['<pad>']\n",
        "\n",
        "dropout_model = initialize_transformer_model(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
        "\n",
        "optimizer = optim.Adam(dropout_model.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Initialize the loss function with CrossEntropyLoss, ignoring the padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "metadata": {
        "id": "dsGh_uzMedZ4"
      },
      "id": "dsGh_uzMedZ4",
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader for the training data, using the generate_batch function as the collate_fn.\n",
        "# This allows custom processing of each batch (adding BOS/EOS tokens and padding) before being fed into the model.\n",
        "train_iterator = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
        "\n",
        "# Similarly, DataLoader for the validation data\n",
        "valid_iterator = DataLoader(valid_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)"
      ],
      "metadata": {
        "id": "vMTOsyseekzA"
      },
      "id": "vMTOsyseekzA",
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 20\n",
        "print(\"Training the model for dropout rate: 0.2\")\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(dropout_model, train_iterator, optimizer, criterion, grad_clip)\n",
        "    val_loss = evaluate(dropout_model, valid_iterator, criterion)\n",
        "\n",
        "    print(f'\\nEpoch: {epoch + 1}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tVal Loss: {val_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gA5wwpA-eqHj",
        "outputId": "6c3bdab3-2a0a-4351-e14e-68061e82233c"
      },
      "id": "gA5wwpA-eqHj",
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model for dropout rate: 0.2\n",
            "\n",
            "Epoch: 1\n",
            "\tTrain Loss: 5.773\n",
            "\tVal Loss: 5.097\n",
            "\n",
            "Epoch: 2\n",
            "\tTrain Loss: 4.955\n",
            "\tVal Loss: 4.838\n",
            "\n",
            "Epoch: 3\n",
            "\tTrain Loss: 4.758\n",
            "\tVal Loss: 4.729\n",
            "\n",
            "Epoch: 4\n",
            "\tTrain Loss: 4.585\n",
            "\tVal Loss: 4.480\n",
            "\n",
            "Epoch: 5\n",
            "\tTrain Loss: 4.301\n",
            "\tVal Loss: 4.182\n",
            "\n",
            "Epoch: 6\n",
            "\tTrain Loss: 4.050\n",
            "\tVal Loss: 3.992\n",
            "\n",
            "Epoch: 7\n",
            "\tTrain Loss: 3.898\n",
            "\tVal Loss: 3.890\n",
            "\n",
            "Epoch: 8\n",
            "\tTrain Loss: 3.785\n",
            "\tVal Loss: 3.834\n",
            "\n",
            "Epoch: 9\n",
            "\tTrain Loss: 3.690\n",
            "\tVal Loss: 3.745\n",
            "\n",
            "Epoch: 10\n",
            "\tTrain Loss: 3.599\n",
            "\tVal Loss: 3.688\n",
            "\n",
            "Epoch: 11\n",
            "\tTrain Loss: 3.528\n",
            "\tVal Loss: 3.655\n",
            "\n",
            "Epoch: 12\n",
            "\tTrain Loss: 3.468\n",
            "\tVal Loss: 3.581\n",
            "\n",
            "Epoch: 13\n",
            "\tTrain Loss: 3.413\n",
            "\tVal Loss: 3.544\n",
            "\n",
            "Epoch: 14\n",
            "\tTrain Loss: 3.362\n",
            "\tVal Loss: 3.537\n",
            "\n",
            "Epoch: 15\n",
            "\tTrain Loss: 3.317\n",
            "\tVal Loss: 3.489\n",
            "\n",
            "Epoch: 16\n",
            "\tTrain Loss: 3.272\n",
            "\tVal Loss: 3.462\n",
            "\n",
            "Epoch: 17\n",
            "\tTrain Loss: 3.228\n",
            "\tVal Loss: 3.423\n",
            "\n",
            "Epoch: 18\n",
            "\tTrain Loss: 3.186\n",
            "\tVal Loss: 3.398\n",
            "\n",
            "Epoch: 19\n",
            "\tTrain Loss: 3.145\n",
            "\tVal Loss: 3.360\n",
            "\n",
            "Epoch: 20\n",
            "\tTrain Loss: 3.105\n",
            "\tVal Loss: 3.356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
        "translated_sentence = translate_sentence(dropout_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqQEjfPWhMaJ",
        "outputId": "9a11d6d2-f77d-4e07-9a2e-7f4709e4208f"
      },
      "id": "NqQEjfPWhMaJ",
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: A boy A boy A boy A boy A boy A boy A boy A boy A boy A boy A boy A boy A boy A boy A boy A boy A boy A boy A boy A boy A boy A boy A boy A boy A boy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Die Sonne scheint hell am blauen Himmel.\"  # German for \"The sun is shining brightly in the blue sky.\"\n",
        "translated_sentence = translate_sentence(dropout_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKbSbgoXhSvi",
        "outputId": "5f472a70-a842-4f17-a755-11be7001ce14"
      },
      "id": "dKbSbgoXhSvi",
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Gib mir bitte das Salz und den Pfeffer.\"  # German for \"Please pass me the salt and pepper.\"\n",
        "translated_sentence = translate_sentence(dropout_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6vxfb0XhYdT",
        "outputId": "fc03d7bf-e860-4d8f-d980-c1b6dfee54c2"
      },
      "id": "e6vxfb0XhYdT",
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Gib mir bitte das Salz und den Pfeffer.\"  # German for \"Please pass me the salt and pepper.\"\n",
        "translated_sentence = translate_sentence(dropout_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLsMbaA9jQF7",
        "outputId": "9c4d8505-49d3-40ab-9d67-b372956d3d96"
      },
      "id": "RLsMbaA9jQF7",
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People People\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Guten Tag.\"  # German for \"Good Morning.\"\n",
        "translated_sentence = translate_sentence(dropout_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t32hpAVyjTAA",
        "outputId": "5ed5c30b-e5c1-4dae-b05c-b07788eda4a1"
      },
      "id": "t32hpAVyjTAA",
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: A dog A dog A dog A dog A dog A dog A dog A dog A dog A dog A dog A dog A dog A dog A dog A dog A dog A dog A dog A dog A dog A dog A dog A dog A dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Changing no of heads to 4"
      ],
      "metadata": {
        "id": "VIQbGCcEhiua"
      },
      "id": "VIQbGCcEhiua"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameters of the model\n",
        "src_vocab_size = len(vocab_src)  # Size of source vocabulary\n",
        "tgt_vocab_size = len(vocab_tgt)  # Size of target vocabulary\n",
        "d_model = 512  # Embedding dimension\n",
        "N = 6          # Number of encoder and decoder layers\n",
        "num_heads = 4  # Number of attention heads\n",
        "d_ff = 2048    # Dimension of feed forward networks\n",
        "max_seq_length = 5000 # Maximum sequence length\n",
        "dropout = 0.1  # Dropout rate\n",
        "learning_rate = 0.0001 #Learning Rate\n",
        "batch_size = 128\n",
        "grad_clip = 1\n",
        "\n",
        "# Assume pad_idx is the padding index in the target vocabulary\n",
        "pad_idx = vocab_tgt['<pad>']\n",
        "\n",
        "head_model = initialize_transformer_model(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
        "\n",
        "optimizer = optim.Adam(head_model.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Initialize the loss function with CrossEntropyLoss, ignoring the padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "metadata": {
        "id": "9PmD-Aflh7rq"
      },
      "id": "9PmD-Aflh7rq",
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader for the training data, using the generate_batch function as the collate_fn.\n",
        "# This allows custom processing of each batch (adding BOS/EOS tokens and padding) before being fed into the model.\n",
        "train_iterator = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
        "\n",
        "# Similarly, DataLoader for the validation data\n",
        "valid_iterator = DataLoader(valid_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)"
      ],
      "metadata": {
        "id": "iUMelGP5isu5"
      },
      "id": "iUMelGP5isu5",
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 20\n",
        "print(\"Training the model for dropout rate: 0.2\")\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(head_model, train_iterator, optimizer, criterion, grad_clip)\n",
        "    val_loss = evaluate(head_model, valid_iterator, criterion)\n",
        "\n",
        "    print(f'\\nEpoch: {epoch + 1}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tVal Loss: {val_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2IE4GQPitY4",
        "outputId": "cf7c3f5c-0795-457a-dc83-f8c07e2a8b2e"
      },
      "id": "I2IE4GQPitY4",
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model for dropout rate: 0.2\n",
            "\n",
            "Epoch: 1\n",
            "\tTrain Loss: 5.713\n",
            "\tVal Loss: 4.991\n",
            "\n",
            "Epoch: 2\n",
            "\tTrain Loss: 4.863\n",
            "\tVal Loss: 4.800\n",
            "\n",
            "Epoch: 3\n",
            "\tTrain Loss: 4.723\n",
            "\tVal Loss: 4.702\n",
            "\n",
            "Epoch: 4\n",
            "\tTrain Loss: 4.532\n",
            "\tVal Loss: 4.388\n",
            "\n",
            "Epoch: 5\n",
            "\tTrain Loss: 4.206\n",
            "\tVal Loss: 4.118\n",
            "\n",
            "Epoch: 6\n",
            "\tTrain Loss: 3.983\n",
            "\tVal Loss: 3.955\n",
            "\n",
            "Epoch: 7\n",
            "\tTrain Loss: 3.832\n",
            "\tVal Loss: 3.885\n",
            "\n",
            "Epoch: 8\n",
            "\tTrain Loss: 3.715\n",
            "\tVal Loss: 3.751\n",
            "\n",
            "Epoch: 9\n",
            "\tTrain Loss: 3.617\n",
            "\tVal Loss: 3.681\n",
            "\n",
            "Epoch: 10\n",
            "\tTrain Loss: 3.524\n",
            "\tVal Loss: 3.605\n",
            "\n",
            "Epoch: 11\n",
            "\tTrain Loss: 3.434\n",
            "\tVal Loss: 3.560\n",
            "\n",
            "Epoch: 12\n",
            "\tTrain Loss: 3.353\n",
            "\tVal Loss: 3.487\n",
            "\n",
            "Epoch: 13\n",
            "\tTrain Loss: 3.274\n",
            "\tVal Loss: 3.432\n",
            "\n",
            "Epoch: 14\n",
            "\tTrain Loss: 3.204\n",
            "\tVal Loss: 3.376\n",
            "\n",
            "Epoch: 15\n",
            "\tTrain Loss: 3.141\n",
            "\tVal Loss: 3.343\n",
            "\n",
            "Epoch: 16\n",
            "\tTrain Loss: 3.080\n",
            "\tVal Loss: 3.304\n",
            "\n",
            "Epoch: 17\n",
            "\tTrain Loss: 3.023\n",
            "\tVal Loss: 3.250\n",
            "\n",
            "Epoch: 18\n",
            "\tTrain Loss: 2.965\n",
            "\tVal Loss: 3.213\n",
            "\n",
            "Epoch: 19\n",
            "\tTrain Loss: 2.909\n",
            "\tVal Loss: 3.176\n",
            "\n",
            "Epoch: 20\n",
            "\tTrain Loss: 2.855\n",
            "\tVal Loss: 3.124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
        "translated_sentence = translate_sentence(head_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvbgarUDjwr2",
        "outputId": "a3098767-503c-45e8-fc6a-3b4ff0a2526f"
      },
      "id": "pvbgarUDjwr2",
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: A child A child A child A child A child A child A child A child A child A child A child A child A child A child A child A child A child A child A child A child A child A child A child A child A child\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Die Sonne scheint hell am blauen Himmel.\"  # German for \"The sun is shining brightly in the blue sky.\"\n",
        "translated_sentence = translate_sentence(head_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ey09glmTjpzr",
        "outputId": "ddb512af-6847-45c3-b3a6-3527964e6a3e"
      },
      "id": "Ey09glmTjpzr",
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: People on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Gib mir bitte das Salz und den Pfeffer.\"  # German for \"Please pass me the salt and pepper.\"\n",
        "translated_sentence = translate_sentence(head_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNb74rlFjnSl",
        "outputId": "126780e7-c5fa-457d-c910-7d34480a9e6b"
      },
      "id": "SNb74rlFjnSl",
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction Construction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Guten Tag.\"  # German for \"Good Morning.\"\n",
        "translated_sentence = translate_sentence(head_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouBQiLn_jcby",
        "outputId": "02f4879d-a394-4df0-c228-028f97a54b10"
      },
      "id": "ouBQiLn_jcby",
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: A <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Changing number of layers to 8"
      ],
      "metadata": {
        "id": "w30GUW0trPVy"
      },
      "id": "w30GUW0trPVy"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameters of the model\n",
        "src_vocab_size = len(vocab_src)  # Size of source vocabulary\n",
        "tgt_vocab_size = len(vocab_tgt)  # Size of target vocabulary\n",
        "d_model = 512  # Embedding dimension\n",
        "N = 8         # Number of encoder and decoder layers\n",
        "num_heads = 8  # Number of attention heads\n",
        "d_ff = 2048    # Dimension of feed forward networks\n",
        "max_seq_length = 5000 # Maximum sequence length\n",
        "dropout = 0.1  # Dropout rate\n",
        "learning_rate = 0.0001 #Learning Rate\n",
        "batch_size = 128\n",
        "grad_clip = 1\n",
        "\n",
        "# Assume pad_idx is the padding index in the target vocabulary\n",
        "pad_idx = vocab_tgt['<pad>']\n",
        "\n",
        "layer_model = initialize_transformer_model(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
        "\n",
        "optimizer = optim.Adam(layer_model.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Initialize the loss function with CrossEntropyLoss, ignoring the padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "metadata": {
        "id": "Uo6_grZ_rSol"
      },
      "id": "Uo6_grZ_rSol",
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader for the training data, using the generate_batch function as the collate_fn.\n",
        "# This allows custom processing of each batch (adding BOS/EOS tokens and padding) before being fed into the model.\n",
        "train_iterator = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
        "\n",
        "# Similarly, DataLoader for the validation data\n",
        "valid_iterator = DataLoader(valid_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)"
      ],
      "metadata": {
        "id": "d97-wHRFrXtF"
      },
      "id": "d97-wHRFrXtF",
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 20\n",
        "print(\"Training the model for 8 layers\")\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(layer_model, train_iterator, optimizer, criterion, grad_clip)\n",
        "    val_loss = evaluate(layer_model, valid_iterator, criterion)\n",
        "\n",
        "    print(f'\\nEpoch: {epoch + 1}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tVal Loss: {val_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2-t1XrurgqN",
        "outputId": "de4b4422-39e4-4b94-e93a-d6146d5455c3"
      },
      "id": "_2-t1XrurgqN",
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model for 8 layers\n",
            "\n",
            "Epoch: 1\n",
            "\tTrain Loss: 5.570\n",
            "\tVal Loss: 5.200\n",
            "\n",
            "Epoch: 2\n",
            "\tTrain Loss: 5.171\n",
            "\tVal Loss: 5.221\n",
            "\n",
            "Epoch: 3\n",
            "\tTrain Loss: 5.107\n",
            "\tVal Loss: 5.177\n",
            "\n",
            "Epoch: 4\n",
            "\tTrain Loss: 5.058\n",
            "\tVal Loss: 5.168\n",
            "\n",
            "Epoch: 5\n",
            "\tTrain Loss: 4.973\n",
            "\tVal Loss: 5.426\n",
            "\n",
            "Epoch: 6\n",
            "\tTrain Loss: 4.930\n",
            "\tVal Loss: 5.424\n",
            "\n",
            "Epoch: 7\n",
            "\tTrain Loss: 4.909\n",
            "\tVal Loss: 5.562\n",
            "\n",
            "Epoch: 8\n",
            "\tTrain Loss: 4.907\n",
            "\tVal Loss: 6.405\n",
            "\n",
            "Epoch: 9\n",
            "\tTrain Loss: 4.854\n",
            "\tVal Loss: 5.708\n",
            "\n",
            "Epoch: 10\n",
            "\tTrain Loss: 4.758\n",
            "\tVal Loss: 5.619\n",
            "\n",
            "Epoch: 11\n",
            "\tTrain Loss: 4.707\n",
            "\tVal Loss: 5.598\n",
            "\n",
            "Epoch: 12\n",
            "\tTrain Loss: 4.671\n",
            "\tVal Loss: 5.821\n",
            "\n",
            "Epoch: 13\n",
            "\tTrain Loss: 4.645\n",
            "\tVal Loss: 5.761\n",
            "\n",
            "Epoch: 14\n",
            "\tTrain Loss: 4.628\n",
            "\tVal Loss: 5.579\n",
            "\n",
            "Epoch: 15\n",
            "\tTrain Loss: 4.624\n",
            "\tVal Loss: 5.505\n",
            "\n",
            "Epoch: 16\n",
            "\tTrain Loss: 4.594\n",
            "\tVal Loss: 5.764\n",
            "\n",
            "Epoch: 17\n",
            "\tTrain Loss: 4.575\n",
            "\tVal Loss: 5.670\n",
            "\n",
            "Epoch: 18\n",
            "\tTrain Loss: 4.580\n",
            "\tVal Loss: 6.745\n",
            "\n",
            "Epoch: 19\n",
            "\tTrain Loss: 4.574\n",
            "\tVal Loss: 5.892\n",
            "\n",
            "Epoch: 20\n",
            "\tTrain Loss: 4.500\n",
            "\tVal Loss: 5.754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
        "translated_sentence = translate_sentence(layer_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "di_OM3tbr9E4",
        "outputId": "29737c7f-8981-407a-c355-a2e66e257874"
      },
      "id": "di_OM3tbr9E4",
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Die Sonne scheint hell am blauen Himmel.\"  # German for \"The sun is shining brightly in the blue sky.\"\n",
        "translated_sentence = translate_sentence(layer_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPchAUZHr-Tb",
        "outputId": "e3b45d72-f18e-4669-fd16-a25f856923c8"
      },
      "id": "iPchAUZHr-Tb",
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Gib mir bitte das Salz und den Pfeffer.\"  # German for \"Please pass me the salt and pepper.\"\n",
        "translated_sentence = translate_sentence(layer_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlqz0s72rjoZ",
        "outputId": "160df562-3466-413a-a65e-cede12f44577"
      },
      "id": "mlqz0s72rjoZ",
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several Several\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Guten Tag.\"  # German for \"Good Morning.\"\n",
        "translated_sentence = translate_sentence(layer_model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4T2L5gvwsMac",
        "outputId": "48720d33-b95b-45f6-9827-84db87af53c2"
      },
      "id": "4T2L5gvwsMac",
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two Two\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Report"
      ],
      "metadata": {
        "id": "Nq60cz42u754"
      },
      "id": "Nq60cz42u754"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task** : Using the transformers for German to English translation.\n",
        "\n",
        "**Dataset**: Multi 30K\n",
        "\n",
        "**Implementation:** Implemented the transformer architecture with encoder-decoder layers, positional encoding and attention mechanism. The code blocks were completed from the reference annotated paper - https://nlp.seas.harvard.edu/annotated-transformer/\n",
        "Following steps were taken while implementing transformers:\n",
        "  \n",
        "  1) Data Preprocessing - Data was tokenized using the tokenizers from the spacy library.\n",
        "  \n",
        "  2) Padding the sentences and appending the <eos>/<bos> tokens to the sentences.\n",
        "  \n",
        "  3) Defining the hyperparameters for the model such as batch_size, learning_rate etc.\n",
        "  \n",
        "  4) Training the model for ~20 epochs and while monitoring the training and validation loss.\n",
        "  \n",
        "  5) Finally using the model to translate some real life german sentences.\n",
        "\n",
        "  6) For the translation both - the greedy approach and the beam search were implemented.\n",
        "\n",
        "\n",
        "  **Results**\n",
        "  - So the transformer model with attention mechanism was trained for 20 epochs using the parameters:\n",
        "    - src_vocab_size = len(vocab_src)  # Size of source vocabulary\n",
        "    - tgt_vocab_size = len(vocab_tgt)  # Size of target vocabulary\n",
        "    - d_model = 512  # Embedding dimension\n",
        "    - N = 8         # Number of encoder and decoder layers\n",
        "    - num_heads = 8  # Number of attention heads\n",
        "    - d_ff = 2048    # Dimension of feed forward networks\n",
        "    - max_seq_length = 5000 # Maximum sequence length\n",
        "    - dropout = 0.1  # Dropout rate\n",
        "    - learning_rate = 0.0001 #Learning Rate\n",
        "    - batch_size = 128\n",
        "    - grad_clip = 1\n",
        "  \n",
        "  After training on these default parameters, the model was used to translate german sentences using greedy approach and beam search.\n",
        "  Following are the results:\n",
        "\n",
        "  **Greedy Approach**\n",
        "\n",
        "  G: Ein kleiner Junge spielt draußen mit einem Ball\n",
        "  \n",
        "  E: A little person in field A little person in field A little person in field A little person in field A little person in field A little person in field A little person in field A little person in field A little person in field A little person in field.\n",
        "\n",
        "  **Beam Search**\n",
        "\n",
        "  G: Ein kleiner Junge spielt draußen mit einem Ball\n",
        "  \n",
        "  E: tag(BOS) A person is playing in the water . tag(EOS)\n",
        "\n",
        "  From the above example, we can clearly see that the Beam search approach is better than the greedy approach.\n",
        "\n",
        "  **After this initial test, some hyperparameters were changed and then the results were observed.**\n",
        "\n",
        "  **1) Increasing No of Epochs** - From 20 to 30\n",
        "   - Training the model for more epochs decreased the training  and validation loss further to 2.4. But this is not guaranteed always as we increase the epochs, there is always a chance that the model may get stuck on a global maxima and then not improve further.\n",
        "\n",
        "   - There was not much improvement in the actual translated sentences though.\n",
        "\n",
        "   **2) Increasing/Decreasing the batch size** - 64, 128 and 256.\n",
        "   - Tweaking the batch size parameter, in both ways gave us good results. Reducing the  batch size especially gave us good results and even a lesser loss value.\n",
        "\n",
        "   **3) Increasing the learning rate** - 0.0001 to 0.0005.\n",
        "   - Tweaking this parameter, did not us good results at all. Increasing the learning rate by just a few points led to a very high validation loss and as a result, the translated sentences are also not so good.\n",
        "\n",
        "   **4) Increasing the dropout rate** - 0.1 to 0.2.\n",
        "   - This initially decreased the training and validation loss but then the loss values plateaued at 4.0. There was no significant change in the actual translated sentences.\n",
        "\n",
        "   **5) Decreasing the number of attention heads** - 8 to 4\n",
        "   - The transformer model performed well even after decreasing the number of attention heads. The loss values for 8 and 4 attention heads were exactly the same and even the translated sentences were similar.\n",
        "\n",
        "   **6) Increasing the number of layers** - 6 to 8.\n",
        "    - When the no of layers was increased from 6 to 8, the training loss plateaued at 4.0. The model didn't show any improvement, rather the quality of the translated sentences was not good.\n",
        "\n",
        "\n",
        "**Challenges**\n",
        "- Implementing the transformer architecture using the reference. One has to have a really good understanding of transformers to implement and debug the code.\n",
        "\n",
        "- Tweaking the hyperparameters to the right value because even the slightest change can cause a big impact on the results."
      ],
      "metadata": {
        "id": "QVRN4MGlu93f"
      },
      "id": "QVRN4MGlu93f"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}